{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IoT-DQA","text":"<p>The IoT-DQA library is a Python package designed to streamline Data Quality Assessment (DQA) for IoT time-series data. It provides robust tools for validating and analyzing IoT data streams, ensuring reliable data for downstream applications.</p> <p>Documentation: https://jeafreezy.github.io/iot-dqa/</p> <p>Source Code: https://github.com/jeafreezy/iot-dqa</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Optimized Performance: Handles large-scale IoT datasets efficiently, powered by the high-performance Polars library.</li> <li>Streamlined Validation: Simplifies the process of validating and analyzing IoT data streams.</li> <li>Custom Metrics: Tailor metrics to meet specific requirements.</li> <li>Comprehensive Scoring: Generates detailed data quality scores across multiple dimensions.</li> <li>Seamless Integration: Export results in formats like CSV and GeoJSON for easy integration with other tools.</li> </ul>"},{"location":"#dimensions-of-data-quality","title":"Dimensions of Data Quality","text":"<ul> <li>Validity: Verifies data adherence to expected formats and ranges.</li> <li>Accuracy: Identifies and quantifies outliers using advanced techniques.</li> <li>Completeness: Evaluates the presence of missing or null values.</li> <li>Timeliness: Measures data arrival punctuality based on timestamps.</li> </ul>"},{"location":"#note","title":"Note:","text":"<ul> <li>Designed for cumulative time-series data (e.g., utility consumption).</li> <li>Sample data is available in <code>tests/test_data.csv</code>.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install iot-dqa\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#example-calculate-data-quality-score-for-iot-time-series-data","title":"Example: Calculate Data Quality Score for IoT time-series data","text":"<pre><code>from iot_dqa import DataQualityScore, Dimension, OutlierDetectionAlgorithm, CompletenessStrategy\n\n# Initialize and compute the Data Quality Score\ndq_score = DataQualityScore(\n    \"./data/sample_data.csv\",\n    multiple_devices=True,\n    dimensions=[\n        Dimension.VALIDITY.value,\n        Dimension.ACCURACY.value,\n        Dimension.COMPLETENESS.value,\n        Dimension.TIMELINESS.value,\n    ],\n    col_mapping={\n        \"latitude\": \"LAT\",\n        \"longitude\": \"LONG\",\n        \"date\": \"DATE\",\n        \"value\": \"VALUE\",\n        \"id\": \"DEVICE_ID\",\n    },\n    metrics_config={\n        \"timeliness\": {\"iat_method\": \"min\"},\n        \"accuracy\": {\n            \"ensemble\": True,\n            \"strategy\": \"validity\",\n            \"algorithms\": [\n                OutlierDetectionAlgorithm.IF.value,\n                OutlierDetectionAlgorithm.IQR.value,\n                OutlierDetectionAlgorithm.MAD.value,\n            ],\n        },\n        \"completeness_strategy\": CompletenessStrategy.ONLY_NULLS.value,\n    },\n).compute_score(\n    weighting_mechanism=\"ahp\",\n    output_format=\"geojson\",\n    output_path=\"./output\",\n    ahp_weights={\n        Dimension.VALIDITY.value: 0.3,\n        Dimension.ACCURACY.value: 0.3,\n        Dimension.COMPLETENESS.value: 0.3,\n        Dimension.TIMELINESS.value: 0.1,\n    },\n)\n\nprint(\"Data Quality Score computed successfully!\")\n</code></pre>"},{"location":"#configuration-overview","title":"Configuration Overview","text":"Configuration Attribute Default Value Description Isolation Forest <code>n_estimators</code> <code>100</code> Number of trees in the forest. <code>max_samples</code> <code>0.8</code> Proportion of samples for training each base estimator. <code>contamination</code> <code>0.1</code> Proportion of outliers in the dataset. <code>max_features</code> <code>1</code> Number of features for training each base estimator. <code>random_state</code> <code>42</code> Random seed for reproducibility. Accuracy <code>ensemble</code> <code>True</code> Use ensemble methods for accuracy. <code>mad_threshold</code> <code>3</code> Threshold for Median Absolute Deviation (MAD). <code>optimize_iqr_with_optuna</code> <code>True</code> Enable IQR optimization using Optuna. <code>iqr_optuna_q1_max</code> <code>0.5</code> Maximum value for Q1 in IQR optimization. <code>iqr_optuna_q3_min</code> <code>0.5</code> Minimum value for Q3 in IQR optimization. <code>iqr_optuna_q3_max</code> <code>1</code> Maximum value for Q3 in IQR optimization. <code>algorithms</code> All algorithms List of outlier detection algorithms. <code>strategy</code> <code>NONE</code> Strategy for accuracy computation. Timeliness <code>iat_method</code> <code>min</code> Method to calculate inter-arrival time. Completeness <code>completeness_strategy</code> <code>ONLY_NULLS</code> Strategy for handling completeness. <p>For more details on configuration, refer to the documentation.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Visit the documentation for comprehensive details.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! See CONTRIBUTING.md for guidelines.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for more information.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#v001-2025-06-04","title":"v0.0.1 - 2025-06-04","text":"<ul> <li>Initial release with core features. See documentation for more information.</li> </ul>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":"<p>This project adheres to the Python Code of Conduct, which can be found here.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to iot_dqa. If you're a developer, you can skip to the development section.</p>"},{"location":"contributing/#general","title":"General","text":"<p>If you're not a developer or you don't have the time to contribute, here are other easy ways to support the project:</p> <ul> <li>Star the project.</li> <li>Tweet about it or promote it on social media.</li> <li>Use it in your project.</li> <li>Mention the project at your organization, conference, articles, or tell your friends/colleagues about it.</li> </ul>"},{"location":"contributing/#development","title":"Development","text":""},{"location":"contributing/#clone-the-project","title":"Clone the project","text":"<pre><code>   # clone the repo\n   git clone https://github.com/jeafreezy/iot-dqa.git\n\n   # enter the project directory\n   cd iot-dqa\n</code></pre>"},{"location":"contributing/#python","title":"Python","text":"<p>This project is developed with Python 3.8.1 and uses Poetry to manage Python dependencies.</p> <p>After cloning the project and installing Poetry, run:</p> <pre><code>   poetry install\n</code></pre> <p>to install all dependencies.</p>"},{"location":"contributing/#pre-commit","title":"Pre-commit","text":"<p>This repo is set to use pre-commit to run the following hooks: check-yaml, end-of-file-fixer, trailing-whitespace, Ruff (\"An extremely fast Python linter and code formatter, written in Rust.\") when committing new code.</p> <p>Run:</p> <p><pre><code>   pre-commit install\n</code></pre> to install the pre-commit hooks.</p> <p>In case you run into errors, it is likely that you haven't installed the dev dependencies. In this case, you can run the command below to install them, then you can retry the command above again.</p> <pre><code>   poetry install --with dev\n</code></pre>"},{"location":"contributing/#tests","title":"Tests","text":"<p>This project uses Pytest for testing. Run the command below to run the tests:</p> <pre><code>   pytest\n</code></pre> <p>In case you run into errors, it is likely that you haven't installed the dev dependencies. In this case, you can run the command below to install them, then you can retry the command above again.</p> <pre><code>   poetry install --with dev\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>The documentation website is generated with mkdocs-material. After poetry install, you can serve the docs website locally with:</p> <pre><code>   poetry run mkdocs serve\n</code></pre> <p>In case you run into errors, it is likely that you haven't installed the docs dependencies. In this case, you can run the command below to install them, then you can retry the command above again.</p> <pre><code>   poetry install --with docs\n</code></pre>"},{"location":"contributing/#ci-cd-publishing","title":"CI-CD &amp; Publishing","text":"<p>This project uses GitHub Actions for automated tests and deployment. The <code>.github/workflows</code> folder comprise of three major workflows: 1. <code>deploy-mkdocs.yml</code> : It handles the automated deployment of the documentation site. It is triggered on every push to the main branch. 2. <code>release.yml</code>: It handles the release of the package to PyPI. It happens when a tag with <code>v*</code> is set and pushed to the main branch. 3. <code>tests.yml</code>: It runs on every pull requests and push to the main branch. It runs the test suites across major Python versions (3.8, 3.9, 3.10, 3.11).</p>"},{"location":"contributing/#helpful-links","title":"Helpful Links","text":"<p>Issues and pull requests are more than welcome:</p> <ul> <li>Project Documentation</li> <li>Issue Tracker</li> <li>Pull Requests</li> <li>Code of Conduct</li> </ul>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2025 Emmanuel Jolaiya</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/dqa/dimensions/","title":"Dimensions","text":""},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Accuracy","title":"<code>Accuracy</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>class Accuracy(BaseMetric):\n    @timer\n    def median_absolute_deviation(self, df_with_outliers: pl.DataFrame) -&gt; pl.DataFrame:\n        \"\"\"\n        Calculate the Median Absolute Deviation (MAD) for outlier detection.\n        This method computes the MAD for the specified column in the DataFrame.\n        If multiple devices are present, it calculates the MAD for each device\n        separately and concatenates the results. The MAD outliers are identified\n        based on a modified Z-score and using optuna specified threshold.\n        Returns:\n            pl.DataFrame: A DataFrame with an additional column \"MAD_outliers\"\n            indicating the presence of outliers (1 for outlier, 0 for non-outlier).\n        \"\"\"\n        accuracy_config = self.config.accuracy\n\n        mad_outliers = df_with_outliers if df_with_outliers is not None else self.df\n        median = self.df[self.col_mapping[ColumnMappingColumnName.VALUE.value]].median()\n        mad = (\n            (self.df[self.col_mapping[ColumnMappingColumnName.VALUE.value]] - median)\n            .abs()\n            .median()\n        )\n\n        if self.multiple_devices:\n            group_results = []\n            for device_id, group in mad_outliers.group_by(\n                self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n            ):\n                logger.info(f\"Detecting MAD outliers for Device: **{device_id[0]}**\")\n                median = group[\n                    self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                ].median()\n                mad = (\n                    (\n                        group[self.col_mapping[ColumnMappingColumnName.VALUE.value]]\n                        - median\n                    )\n                    .abs()\n                    .median()\n                )\n                modified_z_score = (\n                    0.6745\n                    * (\n                        group[self.col_mapping[ColumnMappingColumnName.VALUE.value]]\n                        - median\n                    )\n                    / mad\n                )\n                outliers = (\n                    modified_z_score.abs() &gt; accuracy_config.mad_threshold\n                ).cast(pl.Int8)\n                group_df = group.with_columns(\n                    pl.Series(ColumnName.MAD_OUTLIERS.value, outliers)\n                )\n                group_results.append(group_df)\n            mad_outliers = pl.concat(group_results)\n        else:\n            modified_z_score = (\n                0.6745\n                * (\n                    mad_outliers[self.col_mapping[ColumnMappingColumnName.VALUE.value]]\n                    - median\n                )\n                / mad\n            )\n            outliers = (modified_z_score.abs() &gt; accuracy_config.mad_threshold).cast(\n                pl.Int8\n            )\n            mad_outliers = mad_outliers.with_columns(\n                pl.Series(ColumnName.MAD_OUTLIERS.value, outliers)\n            )\n\n        logger.info(\n            f\"MAD outliers detected successfully. Basic summary: {mad_outliers[ColumnName.MAD_OUTLIERS.value].value_counts()}\"\n        )\n        return mad_outliers\n\n    @timer\n    def isolation_forest(self, df_with_outliers: pl.DataFrame) -&gt; pl.DataFrame:\n        \"\"\"\n        Detects outliers in the dataset using the Isolation Forest algorithm.\n        This method applies the Isolation Forest algorithm to detect outliers in the dataset.\n        If the dataset contains multiple devices, it processes each device's data separately\n        and concatenates the results. Otherwise, it processes the entire dataset at once.\n        Returns:\n            pl.DataFrame: A DataFrame with an additional column \"IF_outliers\" indicating\n                          the presence of outliers (1 if outlier, 0 if not).\n        Raises:\n            ValueError: If the dataset or column mappings are not properly configured.\n        Notes:\n            - The Isolation Forest is instantiated with a random state of 42 and auto contamination.\n            - The method logs the progress and results of the outlier detection process.\n        \"\"\"\n\n        df_with_IF_outliers = (\n            df_with_outliers if df_with_outliers is not None else self.df\n        )\n        logger.info(\"Instantiating Isolation Forest\")\n\n        iso = IsolationForest(**self.config.accuracy.isolation_forest)\n\n        if self.multiple_devices:\n            group_results = []\n            for device_id, group in df_with_IF_outliers.group_by(\n                self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n            ):\n                logger.info(\n                    f\"Detecting Isolation Forest outliers for Device: **{device_id[0]}**\"\n                )\n                outliers = iso.fit_predict(\n                    group[self.col_mapping[ColumnMappingColumnName.VALUE.value]]\n                    .to_numpy()\n                    .reshape(-1, 1)\n                )\n                outliers = np.where(outliers == -1, 1, 0)\n                group_df = group.with_columns(\n                    pl.Series(ColumnName.IF_OUTLIERS.value, outliers)\n                )\n                group_results.append(group_df)\n\n            df_with_IF_outliers = pl.concat(group_results)\n\n        else:\n            outliers = iso.fit_predict(\n                df_with_IF_outliers[\n                    self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                ]\n                .to_numpy()\n                .reshape(-1, 1)\n            )\n            outliers = np.where(outliers == -1, 1, 0)\n            df_with_IF_outliers = df_with_IF_outliers.with_columns(\n                pl.Series(ColumnName.IF_OUTLIERS.value, outliers)\n            )\n        logger.info(\n            f\"Isolation Forest outliers detected successfully. Basic summary: {df_with_IF_outliers[ColumnName.IF_OUTLIERS.value].value_counts()}\"\n        )\n        return df_with_IF_outliers\n\n    @timer\n    def inter_quartile_range(self, df_with_outliers: pl.DataFrame) -&gt; pl.DataFrame:\n        \"\"\"\n        Detects outliers in the dataset using the Inter-Quartile Range (IQR) method with the help of Optuna for\n        hyperparameter optimization.\n        This method can handle multiple devices by grouping the data based on device IDs and applying the IQR\n        outlier detection for each group separately. It uses Optuna to find the optimal lower and upper quantile\n        bounds instead of fixed cutoffs.\n        Returns:\n            pl.DataFrame: A DataFrame with an additional column \"IQR_outliers\" indicating outliers (1 for outlier,\n            0 for non-outlier).\n        \"\"\"\n        # defaults for IQR\n        best_q1 = 0.25\n        best_q3 = 0.75\n        accuracy_config = self.config.accuracy\n        iqr_outliers = df_with_outliers if df_with_outliers is not None else self.df\n\n        def objective(trial, device_df):\n            q1 = trial.suggest_float(\n                \"q1\",\n                accuracy_config.iqr_optuna_q1_min,\n                accuracy_config.iqr_optuna_q1_max,\n            )\n            q3 = trial.suggest_float(\n                \"q3\",\n                accuracy_config.iqr_optuna_q3_min,\n                accuracy_config.iqr_optuna_q3_max,\n            )\n            iqr = q3 - q1\n            lower_bound = (\n                device_df[\n                    self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                ].quantile(q1)\n                - 1.5 * iqr\n            )\n            upper_bound = (\n                device_df[\n                    self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                ].quantile(q3)\n                + 1.5 * iqr\n            )\n\n            outliers = device_df.with_columns(\n                pl.when(\n                    (\n                        pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                        &lt; lower_bound\n                    )\n                    | (\n                        pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                        &gt; upper_bound\n                    )\n                )\n                .then(1)\n                .otherwise(0)\n                .alias(ColumnName.IQR_OUTLIERS.value)\n            )\n            return outliers[ColumnName.IQR_OUTLIERS.value].sum()\n\n        if self.multiple_devices:\n            group_results = []\n            for device_id, group in iqr_outliers.group_by(\n                self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n            ):\n                logger.info(f\"Detecting IQR outliers for Device: **{device_id[0]}**\")\n                if accuracy_config.optimize_iqr_with_optuna:\n                    study = optuna.create_study(direction=\"minimize\")\n                    study.optimize(\n                        lambda trial: objective(trial, group),\n                        n_trials=accuracy_config.iqr_optuna_trials,\n                    )\n\n                    best_q1 = study.best_params[\"q1\"]\n                    best_q3 = study.best_params[\"q3\"]\n\n                iqr = best_q3 - best_q1\n                lower_bound = (\n                    group[\n                        self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                    ].quantile(best_q1)\n                    - 1.5 * iqr\n                )\n                upper_bound = (\n                    group[\n                        self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                    ].quantile(best_q3)\n                    + 1.5 * iqr\n                )\n\n                group_df = group.with_columns(\n                    pl.when(\n                        (\n                            pl.col(\n                                self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                            )\n                            &lt; lower_bound\n                        )\n                        | (\n                            pl.col(\n                                self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                            )\n                            &gt; upper_bound\n                        )\n                    )\n                    .then(1)\n                    .otherwise(0)\n                    .alias(ColumnName.IQR_OUTLIERS.value)\n                )\n                group_results.append(group_df)\n\n            iqr_outliers = pl.concat(group_results)\n        else:\n            if accuracy_config.optimize_iqr_with_optuna:\n                study = optuna.create_study(direction=\"minimize\")\n                study.optimize(\n                    lambda trial: objective(trial, iqr_outliers),\n                    n_trials=accuracy_config.iqr_optuna_trials,\n                )\n\n                best_q1 = study.best_params[\"q1\"]\n                best_q3 = study.best_params[\"q3\"]\n\n            iqr = best_q3 - best_q1\n            lower_bound = (\n                iqr_outliers[\n                    self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                ].quantile(best_q1)\n                - 1.5 * iqr\n            )\n            upper_bound = (\n                iqr_outliers[\n                    self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                ].quantile(best_q3)\n                + 1.5 * iqr\n            )\n\n            iqr_outliers = iqr_outliers.with_columns(\n                pl.when(\n                    (\n                        pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                        &lt; lower_bound\n                    )\n                    | (\n                        pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                        &gt; upper_bound\n                    )\n                )\n                .then(1)\n                .otherwise(0)\n                .alias(ColumnName.IQR_OUTLIERS.value)\n            )\n\n        logger.info(\n            f\"IQR outliers detected successfully. Basic summary: {iqr_outliers[ColumnName.IQR_OUTLIERS.value].value_counts()}\"\n        )\n        return iqr_outliers\n\n    def compute_metric(self) -&gt; pl.DataFrame:\n        \"\"\"\n        Computes the metric by detecting outliers using specified algorithms.\n        This method checks the configuration for the specified outlier detection\n        algorithms and applies them to the data. The supported algorithms are:\n        Isolation Forest (IF), Inter-Quartile Range (IQR), and Median Absolute\n        Deviation (MAD). The method returns a DataFrame with the detected outliers.\n        Returns:\n            pl.DataFrame: A DataFrame containing the data with detected outliers.\n        \"\"\"\n\n        df_with_outliers = self.df\n        accuracy_config = self.config.accuracy\n        if OutlierDetectionAlgorithm.IF.value in accuracy_config.algorithms:\n            df_with_outliers = self.isolation_forest(df_with_outliers)\n        if OutlierDetectionAlgorithm.IQR.value in accuracy_config.algorithms:\n            df_with_outliers = self.inter_quartile_range(df_with_outliers)\n        if OutlierDetectionAlgorithm.MAD.value in accuracy_config.algorithms:\n            df_with_outliers = self.median_absolute_deviation(df_with_outliers)\n\n        outlier_columns = [\n            ColumnName.MAD_OUTLIERS.value,\n            ColumnName.IF_OUTLIERS.value,\n            ColumnName.IQR_OUTLIERS.value,\n        ]\n        relevant_columns = [\n            col for col in outlier_columns if col in df_with_outliers.columns\n        ]\n\n        if relevant_columns:\n            # Combine outlier columns by taking the maximum value across them\n            df_with_outliers = df_with_outliers.with_columns(\n                pl.max_horizontal([pl.col(col) for col in relevant_columns])\n                .cast(pl.Int8)\n                .alias(ColumnName.ACCURACY.value)\n            )\n        # remove outlier columns\n        for col in outlier_columns:\n            if col in df_with_outliers.columns:\n                df_with_outliers = df_with_outliers.drop(col)\n\n        return df_with_outliers\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Accuracy.compute_metric","title":"<code>compute_metric()</code>","text":"<p>Computes the metric by detecting outliers using specified algorithms. This method checks the configuration for the specified outlier detection algorithms and applies them to the data. The supported algorithms are: Isolation Forest (IF), Inter-Quartile Range (IQR), and Median Absolute Deviation (MAD). The method returns a DataFrame with the detected outliers. Returns:     pl.DataFrame: A DataFrame containing the data with detected outliers.</p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>def compute_metric(self) -&gt; pl.DataFrame:\n    \"\"\"\n    Computes the metric by detecting outliers using specified algorithms.\n    This method checks the configuration for the specified outlier detection\n    algorithms and applies them to the data. The supported algorithms are:\n    Isolation Forest (IF), Inter-Quartile Range (IQR), and Median Absolute\n    Deviation (MAD). The method returns a DataFrame with the detected outliers.\n    Returns:\n        pl.DataFrame: A DataFrame containing the data with detected outliers.\n    \"\"\"\n\n    df_with_outliers = self.df\n    accuracy_config = self.config.accuracy\n    if OutlierDetectionAlgorithm.IF.value in accuracy_config.algorithms:\n        df_with_outliers = self.isolation_forest(df_with_outliers)\n    if OutlierDetectionAlgorithm.IQR.value in accuracy_config.algorithms:\n        df_with_outliers = self.inter_quartile_range(df_with_outliers)\n    if OutlierDetectionAlgorithm.MAD.value in accuracy_config.algorithms:\n        df_with_outliers = self.median_absolute_deviation(df_with_outliers)\n\n    outlier_columns = [\n        ColumnName.MAD_OUTLIERS.value,\n        ColumnName.IF_OUTLIERS.value,\n        ColumnName.IQR_OUTLIERS.value,\n    ]\n    relevant_columns = [\n        col for col in outlier_columns if col in df_with_outliers.columns\n    ]\n\n    if relevant_columns:\n        # Combine outlier columns by taking the maximum value across them\n        df_with_outliers = df_with_outliers.with_columns(\n            pl.max_horizontal([pl.col(col) for col in relevant_columns])\n            .cast(pl.Int8)\n            .alias(ColumnName.ACCURACY.value)\n        )\n    # remove outlier columns\n    for col in outlier_columns:\n        if col in df_with_outliers.columns:\n            df_with_outliers = df_with_outliers.drop(col)\n\n    return df_with_outliers\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Accuracy.inter_quartile_range","title":"<code>inter_quartile_range(df_with_outliers)</code>","text":"<p>Detects outliers in the dataset using the Inter-Quartile Range (IQR) method with the help of Optuna for hyperparameter optimization. This method can handle multiple devices by grouping the data based on device IDs and applying the IQR outlier detection for each group separately. It uses Optuna to find the optimal lower and upper quantile bounds instead of fixed cutoffs. Returns:     pl.DataFrame: A DataFrame with an additional column \"IQR_outliers\" indicating outliers (1 for outlier,     0 for non-outlier).</p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>@timer\ndef inter_quartile_range(self, df_with_outliers: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Detects outliers in the dataset using the Inter-Quartile Range (IQR) method with the help of Optuna for\n    hyperparameter optimization.\n    This method can handle multiple devices by grouping the data based on device IDs and applying the IQR\n    outlier detection for each group separately. It uses Optuna to find the optimal lower and upper quantile\n    bounds instead of fixed cutoffs.\n    Returns:\n        pl.DataFrame: A DataFrame with an additional column \"IQR_outliers\" indicating outliers (1 for outlier,\n        0 for non-outlier).\n    \"\"\"\n    # defaults for IQR\n    best_q1 = 0.25\n    best_q3 = 0.75\n    accuracy_config = self.config.accuracy\n    iqr_outliers = df_with_outliers if df_with_outliers is not None else self.df\n\n    def objective(trial, device_df):\n        q1 = trial.suggest_float(\n            \"q1\",\n            accuracy_config.iqr_optuna_q1_min,\n            accuracy_config.iqr_optuna_q1_max,\n        )\n        q3 = trial.suggest_float(\n            \"q3\",\n            accuracy_config.iqr_optuna_q3_min,\n            accuracy_config.iqr_optuna_q3_max,\n        )\n        iqr = q3 - q1\n        lower_bound = (\n            device_df[\n                self.col_mapping[ColumnMappingColumnName.VALUE.value]\n            ].quantile(q1)\n            - 1.5 * iqr\n        )\n        upper_bound = (\n            device_df[\n                self.col_mapping[ColumnMappingColumnName.VALUE.value]\n            ].quantile(q3)\n            + 1.5 * iqr\n        )\n\n        outliers = device_df.with_columns(\n            pl.when(\n                (\n                    pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                    &lt; lower_bound\n                )\n                | (\n                    pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                    &gt; upper_bound\n                )\n            )\n            .then(1)\n            .otherwise(0)\n            .alias(ColumnName.IQR_OUTLIERS.value)\n        )\n        return outliers[ColumnName.IQR_OUTLIERS.value].sum()\n\n    if self.multiple_devices:\n        group_results = []\n        for device_id, group in iqr_outliers.group_by(\n            self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n        ):\n            logger.info(f\"Detecting IQR outliers for Device: **{device_id[0]}**\")\n            if accuracy_config.optimize_iqr_with_optuna:\n                study = optuna.create_study(direction=\"minimize\")\n                study.optimize(\n                    lambda trial: objective(trial, group),\n                    n_trials=accuracy_config.iqr_optuna_trials,\n                )\n\n                best_q1 = study.best_params[\"q1\"]\n                best_q3 = study.best_params[\"q3\"]\n\n            iqr = best_q3 - best_q1\n            lower_bound = (\n                group[\n                    self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                ].quantile(best_q1)\n                - 1.5 * iqr\n            )\n            upper_bound = (\n                group[\n                    self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                ].quantile(best_q3)\n                + 1.5 * iqr\n            )\n\n            group_df = group.with_columns(\n                pl.when(\n                    (\n                        pl.col(\n                            self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                        )\n                        &lt; lower_bound\n                    )\n                    | (\n                        pl.col(\n                            self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                        )\n                        &gt; upper_bound\n                    )\n                )\n                .then(1)\n                .otherwise(0)\n                .alias(ColumnName.IQR_OUTLIERS.value)\n            )\n            group_results.append(group_df)\n\n        iqr_outliers = pl.concat(group_results)\n    else:\n        if accuracy_config.optimize_iqr_with_optuna:\n            study = optuna.create_study(direction=\"minimize\")\n            study.optimize(\n                lambda trial: objective(trial, iqr_outliers),\n                n_trials=accuracy_config.iqr_optuna_trials,\n            )\n\n            best_q1 = study.best_params[\"q1\"]\n            best_q3 = study.best_params[\"q3\"]\n\n        iqr = best_q3 - best_q1\n        lower_bound = (\n            iqr_outliers[\n                self.col_mapping[ColumnMappingColumnName.VALUE.value]\n            ].quantile(best_q1)\n            - 1.5 * iqr\n        )\n        upper_bound = (\n            iqr_outliers[\n                self.col_mapping[ColumnMappingColumnName.VALUE.value]\n            ].quantile(best_q3)\n            + 1.5 * iqr\n        )\n\n        iqr_outliers = iqr_outliers.with_columns(\n            pl.when(\n                (\n                    pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                    &lt; lower_bound\n                )\n                | (\n                    pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                    &gt; upper_bound\n                )\n            )\n            .then(1)\n            .otherwise(0)\n            .alias(ColumnName.IQR_OUTLIERS.value)\n        )\n\n    logger.info(\n        f\"IQR outliers detected successfully. Basic summary: {iqr_outliers[ColumnName.IQR_OUTLIERS.value].value_counts()}\"\n    )\n    return iqr_outliers\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Accuracy.isolation_forest","title":"<code>isolation_forest(df_with_outliers)</code>","text":"<p>Detects outliers in the dataset using the Isolation Forest algorithm. This method applies the Isolation Forest algorithm to detect outliers in the dataset. If the dataset contains multiple devices, it processes each device's data separately and concatenates the results. Otherwise, it processes the entire dataset at once. Returns:     pl.DataFrame: A DataFrame with an additional column \"IF_outliers\" indicating                   the presence of outliers (1 if outlier, 0 if not). Raises:     ValueError: If the dataset or column mappings are not properly configured. Notes:     - The Isolation Forest is instantiated with a random state of 42 and auto contamination.     - The method logs the progress and results of the outlier detection process.</p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>@timer\ndef isolation_forest(self, df_with_outliers: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Detects outliers in the dataset using the Isolation Forest algorithm.\n    This method applies the Isolation Forest algorithm to detect outliers in the dataset.\n    If the dataset contains multiple devices, it processes each device's data separately\n    and concatenates the results. Otherwise, it processes the entire dataset at once.\n    Returns:\n        pl.DataFrame: A DataFrame with an additional column \"IF_outliers\" indicating\n                      the presence of outliers (1 if outlier, 0 if not).\n    Raises:\n        ValueError: If the dataset or column mappings are not properly configured.\n    Notes:\n        - The Isolation Forest is instantiated with a random state of 42 and auto contamination.\n        - The method logs the progress and results of the outlier detection process.\n    \"\"\"\n\n    df_with_IF_outliers = (\n        df_with_outliers if df_with_outliers is not None else self.df\n    )\n    logger.info(\"Instantiating Isolation Forest\")\n\n    iso = IsolationForest(**self.config.accuracy.isolation_forest)\n\n    if self.multiple_devices:\n        group_results = []\n        for device_id, group in df_with_IF_outliers.group_by(\n            self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n        ):\n            logger.info(\n                f\"Detecting Isolation Forest outliers for Device: **{device_id[0]}**\"\n            )\n            outliers = iso.fit_predict(\n                group[self.col_mapping[ColumnMappingColumnName.VALUE.value]]\n                .to_numpy()\n                .reshape(-1, 1)\n            )\n            outliers = np.where(outliers == -1, 1, 0)\n            group_df = group.with_columns(\n                pl.Series(ColumnName.IF_OUTLIERS.value, outliers)\n            )\n            group_results.append(group_df)\n\n        df_with_IF_outliers = pl.concat(group_results)\n\n    else:\n        outliers = iso.fit_predict(\n            df_with_IF_outliers[\n                self.col_mapping[ColumnMappingColumnName.VALUE.value]\n            ]\n            .to_numpy()\n            .reshape(-1, 1)\n        )\n        outliers = np.where(outliers == -1, 1, 0)\n        df_with_IF_outliers = df_with_IF_outliers.with_columns(\n            pl.Series(ColumnName.IF_OUTLIERS.value, outliers)\n        )\n    logger.info(\n        f\"Isolation Forest outliers detected successfully. Basic summary: {df_with_IF_outliers[ColumnName.IF_OUTLIERS.value].value_counts()}\"\n    )\n    return df_with_IF_outliers\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Accuracy.median_absolute_deviation","title":"<code>median_absolute_deviation(df_with_outliers)</code>","text":"<p>Calculate the Median Absolute Deviation (MAD) for outlier detection. This method computes the MAD for the specified column in the DataFrame. If multiple devices are present, it calculates the MAD for each device separately and concatenates the results. The MAD outliers are identified based on a modified Z-score and using optuna specified threshold. Returns:     pl.DataFrame: A DataFrame with an additional column \"MAD_outliers\"     indicating the presence of outliers (1 for outlier, 0 for non-outlier).</p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>@timer\ndef median_absolute_deviation(self, df_with_outliers: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Calculate the Median Absolute Deviation (MAD) for outlier detection.\n    This method computes the MAD for the specified column in the DataFrame.\n    If multiple devices are present, it calculates the MAD for each device\n    separately and concatenates the results. The MAD outliers are identified\n    based on a modified Z-score and using optuna specified threshold.\n    Returns:\n        pl.DataFrame: A DataFrame with an additional column \"MAD_outliers\"\n        indicating the presence of outliers (1 for outlier, 0 for non-outlier).\n    \"\"\"\n    accuracy_config = self.config.accuracy\n\n    mad_outliers = df_with_outliers if df_with_outliers is not None else self.df\n    median = self.df[self.col_mapping[ColumnMappingColumnName.VALUE.value]].median()\n    mad = (\n        (self.df[self.col_mapping[ColumnMappingColumnName.VALUE.value]] - median)\n        .abs()\n        .median()\n    )\n\n    if self.multiple_devices:\n        group_results = []\n        for device_id, group in mad_outliers.group_by(\n            self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n        ):\n            logger.info(f\"Detecting MAD outliers for Device: **{device_id[0]}**\")\n            median = group[\n                self.col_mapping[ColumnMappingColumnName.VALUE.value]\n            ].median()\n            mad = (\n                (\n                    group[self.col_mapping[ColumnMappingColumnName.VALUE.value]]\n                    - median\n                )\n                .abs()\n                .median()\n            )\n            modified_z_score = (\n                0.6745\n                * (\n                    group[self.col_mapping[ColumnMappingColumnName.VALUE.value]]\n                    - median\n                )\n                / mad\n            )\n            outliers = (\n                modified_z_score.abs() &gt; accuracy_config.mad_threshold\n            ).cast(pl.Int8)\n            group_df = group.with_columns(\n                pl.Series(ColumnName.MAD_OUTLIERS.value, outliers)\n            )\n            group_results.append(group_df)\n        mad_outliers = pl.concat(group_results)\n    else:\n        modified_z_score = (\n            0.6745\n            * (\n                mad_outliers[self.col_mapping[ColumnMappingColumnName.VALUE.value]]\n                - median\n            )\n            / mad\n        )\n        outliers = (modified_z_score.abs() &gt; accuracy_config.mad_threshold).cast(\n            pl.Int8\n        )\n        mad_outliers = mad_outliers.with_columns(\n            pl.Series(ColumnName.MAD_OUTLIERS.value, outliers)\n        )\n\n    logger.info(\n        f\"MAD outliers detected successfully. Basic summary: {mad_outliers[ColumnName.MAD_OUTLIERS.value].value_counts()}\"\n    )\n    return mad_outliers\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.BaseMetric","title":"<code>BaseMetric</code>","text":"Source code in <code>iot_dqa/dimensions.py</code> <pre><code>class BaseMetric:\n    def __init__(\n        self,\n        df: pl.DataFrame,\n        col_mapping: dict[str, str],\n        multiple_devices: bool = False,\n        config: MetricsConfig = None,\n    ):\n        self.df = df\n        self.col_mapping = col_mapping\n        self.multiple_devices = multiple_devices\n        self.config = config\n\n    def compute_metric(self):\n        \"\"\"\n        Calculate the metric for the dimension.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.BaseMetric.compute_metric","title":"<code>compute_metric()</code>","text":"<p>Calculate the metric for the dimension.</p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>def compute_metric(self):\n    \"\"\"\n    Calculate the metric for the dimension.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Completeness","title":"<code>Completeness</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>class Completeness(BaseMetric):\n    @timer\n    def compute_completeness_metrics(self) -&gt; pl.DataFrame:\n        \"\"\"\n        Compute the completeness metrics using different strategy.\n        \"\"\"\n        completeness_df = self.df\n\n        if self.config.completeness_strategy == CompletenessStrategy.ONLY_NULLS.value:\n            completeness_df = completeness_df.with_columns(\n                pl.when(\n                    pl.col(\n                        self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                    ).is_null()\n                )\n                .then(0)\n                .otherwise(1)\n                .alias(ColumnName.COMPLETENESS.value)\n                .cast(pl.Int8)\n            )\n        elif self.config.completeness_strategy == CompletenessStrategy.ACCURACY.value:\n            # if accuracy is not computed, instantiate and compute accuracy here.\n            if ColumnName.ACCURACY.value not in completeness_df.columns:\n                completeness_df = Accuracy(\n                    completeness_df,\n                    self.col_mapping,\n                    self.multiple_devices,\n                    self.config,\n                ).compute_metric()\n\n                completeness_df = completeness_df.with_columns(\n                    pl.when(pl.col(ColumnName.ACCURACY.value) == 0)\n                    .then(0)\n                    .otherwise(1)\n                    .alias(ColumnName.COMPLETENESS.value)\n                    .cast(pl.Int8)\n                )\n                # remove accuracy column\n                completeness_df = completeness_df.drop(ColumnName.ACCURACY.value)\n\n            else:\n                completeness_df = completeness_df.with_columns(\n                    pl.when(pl.col(ColumnName.ACCURACY.value) == 0)\n                    .then(0)\n                    .otherwise(1)\n                    .alias(ColumnName.COMPLETENESS.value)\n                    .cast(pl.Int8)\n                )\n\n        return completeness_df\n\n    def compute_metric(self) -&gt; pl.DataFrame:\n        \"\"\"\n        Computes the completeness metric for the devices.\n        \"\"\"\n        completeness_df = self.compute_completeness_metrics()\n        logger.info(\n            f\"Completeness metric computed successfully: Basic statistics -&gt; {(completeness_df[ColumnName.COMPLETENESS.value].value_counts(),)}\",\n        )\n        return completeness_df\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Completeness.compute_completeness_metrics","title":"<code>compute_completeness_metrics()</code>","text":"<p>Compute the completeness metrics using different strategy.</p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>@timer\ndef compute_completeness_metrics(self) -&gt; pl.DataFrame:\n    \"\"\"\n    Compute the completeness metrics using different strategy.\n    \"\"\"\n    completeness_df = self.df\n\n    if self.config.completeness_strategy == CompletenessStrategy.ONLY_NULLS.value:\n        completeness_df = completeness_df.with_columns(\n            pl.when(\n                pl.col(\n                    self.col_mapping[ColumnMappingColumnName.VALUE.value]\n                ).is_null()\n            )\n            .then(0)\n            .otherwise(1)\n            .alias(ColumnName.COMPLETENESS.value)\n            .cast(pl.Int8)\n        )\n    elif self.config.completeness_strategy == CompletenessStrategy.ACCURACY.value:\n        # if accuracy is not computed, instantiate and compute accuracy here.\n        if ColumnName.ACCURACY.value not in completeness_df.columns:\n            completeness_df = Accuracy(\n                completeness_df,\n                self.col_mapping,\n                self.multiple_devices,\n                self.config,\n            ).compute_metric()\n\n            completeness_df = completeness_df.with_columns(\n                pl.when(pl.col(ColumnName.ACCURACY.value) == 0)\n                .then(0)\n                .otherwise(1)\n                .alias(ColumnName.COMPLETENESS.value)\n                .cast(pl.Int8)\n            )\n            # remove accuracy column\n            completeness_df = completeness_df.drop(ColumnName.ACCURACY.value)\n\n        else:\n            completeness_df = completeness_df.with_columns(\n                pl.when(pl.col(ColumnName.ACCURACY.value) == 0)\n                .then(0)\n                .otherwise(1)\n                .alias(ColumnName.COMPLETENESS.value)\n                .cast(pl.Int8)\n            )\n\n    return completeness_df\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Completeness.compute_metric","title":"<code>compute_metric()</code>","text":"<p>Computes the completeness metric for the devices.</p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>def compute_metric(self) -&gt; pl.DataFrame:\n    \"\"\"\n    Computes the completeness metric for the devices.\n    \"\"\"\n    completeness_df = self.compute_completeness_metrics()\n    logger.info(\n        f\"Completeness metric computed successfully: Basic statistics -&gt; {(completeness_df[ColumnName.COMPLETENESS.value].value_counts(),)}\",\n    )\n    return completeness_df\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Timeliness","title":"<code>Timeliness</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>class Timeliness(BaseMetric):\n    def compute_metric(\n        self,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Computes the Inter-Arrival Time Regularity metric for timeliness assessment.\n\n        Returns:\n            pl.DataFrame: A dataframe containing the timeliness metric per record.\n        \"\"\"\n\n        # Calculate Inter-Arrival Time (IAT)\n        df = self.df.with_columns(\n            (\n                pl.col(self.col_mapping[ColumnMappingColumnName.DATE.value])\n                .cast(pl.Datetime)  # Ensure the column is in datetime format\n                .diff()\n                .over(self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value])\n                .alias(ColumnName.IAT.value)\n                / 1_000_000  # Convert duration to seconds\n            )\n        )\n\n        # Fill null values with the min or mode based on the calculation method, per device\n        if self.config.timeliness.iat_method == FrequencyCalculationMethod.MIN.value:\n            fill_values = df.group_by(\n                self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n            ).agg(pl.col(ColumnName.IAT.value).min().alias(ColumnName.FILL_VALUE.value))\n        elif self.config.timeliness.iat_method == FrequencyCalculationMethod.MODE.value:\n            fill_values = df.group_by(\n                self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n            ).agg(\n                pl.col(ColumnName.IAT.value).mode().alias(ColumnName.FILL_VALUE.value)\n            )\n        else:\n            fill_values = df.select(\n                pl.lit(0).alias(ColumnName.FILL_VALUE.value)\n            )  # Default fallback for all devices\n\n        # Join the fill values back to the original dataframe\n        df = df.join(\n            fill_values,\n            on=self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value],\n            how=\"left\",\n        )\n\n        # Fill null values in IAT column with the per-device fill value\n        df = df.with_columns(\n            pl.col(ColumnName.IAT.value).fill_null(pl.col(ColumnName.FILL_VALUE.value))\n        )\n\n        # Deduce the expected interval based on the iat_method in the configuration\n        if self.config.timeliness.iat_method == FrequencyCalculationMethod.MIN.value:\n            expected_interval = df.group_by(\n                self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n            ).agg(\n                pl.col(ColumnName.IAT.value)\n                .min()\n                .alias(\n                    ColumnName.EXPECTED_INTERVAL.value,\n                )\n            )\n        elif self.config.timeliness.iat_method == FrequencyCalculationMethod.MODE.value:\n            expected_interval = df.group_by(\n                self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n            ).agg(\n                pl.col(ColumnName.IAT.value)\n                .mode()\n                .alias(\n                    ColumnName.EXPECTED_INTERVAL.value,\n                )\n            )\n\n        # Join the expected interval back to the original dataframe\n        df = df.join(\n            expected_interval,\n            on=self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value],\n        )\n\n        # Compute Relative Absolute Error (RAE) based on the deduced expected interval\n        df = df.with_columns(\n            pl.when(pl.col(ColumnName.EXPECTED_INTERVAL.value) != 0)\n            .then(\n                (\n                    (\n                        pl.col(ColumnName.IAT.value)\n                        - pl.col(ColumnName.EXPECTED_INTERVAL.value)\n                    ).abs()\n                    / pl.col(ColumnName.EXPECTED_INTERVAL.value)\n                )\n            )\n            .otherwise(0)\n            .alias(ColumnName.RAE.value)\n        )\n\n        # Compute goodness and penalty scores\n        df = df.with_columns(\n            [\n                pl.when(pl.col(ColumnName.RAE.value) &lt;= 0.5)\n                .then(1 - 2 * pl.col(ColumnName.RAE.value))\n                .otherwise(0)\n                .alias(ColumnName.GOODNESS.value),\n                pl.when(pl.col(ColumnName.RAE.value) &gt; 0.5)\n                .then(2 * pl.col(ColumnName.RAE.value))\n                .otherwise(0)\n                .alias(ColumnName.PENALTY.value),\n            ]\n        )\n\n        # Calculate the timeliness score per record\n        df = df.with_columns(\n            (pl.col(ColumnName.GOODNESS.value) / (1 + pl.col(ColumnName.PENALTY.value)))\n            .cast(pl.Int8)\n            .alias(ColumnName.TIMELINESS.value)\n        )\n\n        # Drop intermediate columns used for calculations\n        df = df.drop(\n            [\n                ColumnName.GOODNESS.value,\n                ColumnName.PENALTY.value,\n                ColumnName.RAE.value,\n                ColumnName.IAT.value,\n                ColumnName.EXPECTED_INTERVAL.value,\n                ColumnName.FILL_VALUE.value,\n            ]\n        )\n        logger.info(\n            f\"Timeliness metric computed successfully: Basic statistics -&gt; {(df[ColumnName.TIMELINESS.value].value_counts(),)}\",\n        )\n        return df\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Timeliness.compute_metric","title":"<code>compute_metric()</code>","text":"<p>Computes the Inter-Arrival Time Regularity metric for timeliness assessment.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A dataframe containing the timeliness metric per record.</p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>def compute_metric(\n    self,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Computes the Inter-Arrival Time Regularity metric for timeliness assessment.\n\n    Returns:\n        pl.DataFrame: A dataframe containing the timeliness metric per record.\n    \"\"\"\n\n    # Calculate Inter-Arrival Time (IAT)\n    df = self.df.with_columns(\n        (\n            pl.col(self.col_mapping[ColumnMappingColumnName.DATE.value])\n            .cast(pl.Datetime)  # Ensure the column is in datetime format\n            .diff()\n            .over(self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value])\n            .alias(ColumnName.IAT.value)\n            / 1_000_000  # Convert duration to seconds\n        )\n    )\n\n    # Fill null values with the min or mode based on the calculation method, per device\n    if self.config.timeliness.iat_method == FrequencyCalculationMethod.MIN.value:\n        fill_values = df.group_by(\n            self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n        ).agg(pl.col(ColumnName.IAT.value).min().alias(ColumnName.FILL_VALUE.value))\n    elif self.config.timeliness.iat_method == FrequencyCalculationMethod.MODE.value:\n        fill_values = df.group_by(\n            self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n        ).agg(\n            pl.col(ColumnName.IAT.value).mode().alias(ColumnName.FILL_VALUE.value)\n        )\n    else:\n        fill_values = df.select(\n            pl.lit(0).alias(ColumnName.FILL_VALUE.value)\n        )  # Default fallback for all devices\n\n    # Join the fill values back to the original dataframe\n    df = df.join(\n        fill_values,\n        on=self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value],\n        how=\"left\",\n    )\n\n    # Fill null values in IAT column with the per-device fill value\n    df = df.with_columns(\n        pl.col(ColumnName.IAT.value).fill_null(pl.col(ColumnName.FILL_VALUE.value))\n    )\n\n    # Deduce the expected interval based on the iat_method in the configuration\n    if self.config.timeliness.iat_method == FrequencyCalculationMethod.MIN.value:\n        expected_interval = df.group_by(\n            self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n        ).agg(\n            pl.col(ColumnName.IAT.value)\n            .min()\n            .alias(\n                ColumnName.EXPECTED_INTERVAL.value,\n            )\n        )\n    elif self.config.timeliness.iat_method == FrequencyCalculationMethod.MODE.value:\n        expected_interval = df.group_by(\n            self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n        ).agg(\n            pl.col(ColumnName.IAT.value)\n            .mode()\n            .alias(\n                ColumnName.EXPECTED_INTERVAL.value,\n            )\n        )\n\n    # Join the expected interval back to the original dataframe\n    df = df.join(\n        expected_interval,\n        on=self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value],\n    )\n\n    # Compute Relative Absolute Error (RAE) based on the deduced expected interval\n    df = df.with_columns(\n        pl.when(pl.col(ColumnName.EXPECTED_INTERVAL.value) != 0)\n        .then(\n            (\n                (\n                    pl.col(ColumnName.IAT.value)\n                    - pl.col(ColumnName.EXPECTED_INTERVAL.value)\n                ).abs()\n                / pl.col(ColumnName.EXPECTED_INTERVAL.value)\n            )\n        )\n        .otherwise(0)\n        .alias(ColumnName.RAE.value)\n    )\n\n    # Compute goodness and penalty scores\n    df = df.with_columns(\n        [\n            pl.when(pl.col(ColumnName.RAE.value) &lt;= 0.5)\n            .then(1 - 2 * pl.col(ColumnName.RAE.value))\n            .otherwise(0)\n            .alias(ColumnName.GOODNESS.value),\n            pl.when(pl.col(ColumnName.RAE.value) &gt; 0.5)\n            .then(2 * pl.col(ColumnName.RAE.value))\n            .otherwise(0)\n            .alias(ColumnName.PENALTY.value),\n        ]\n    )\n\n    # Calculate the timeliness score per record\n    df = df.with_columns(\n        (pl.col(ColumnName.GOODNESS.value) / (1 + pl.col(ColumnName.PENALTY.value)))\n        .cast(pl.Int8)\n        .alias(ColumnName.TIMELINESS.value)\n    )\n\n    # Drop intermediate columns used for calculations\n    df = df.drop(\n        [\n            ColumnName.GOODNESS.value,\n            ColumnName.PENALTY.value,\n            ColumnName.RAE.value,\n            ColumnName.IAT.value,\n            ColumnName.EXPECTED_INTERVAL.value,\n            ColumnName.FILL_VALUE.value,\n        ]\n    )\n    logger.info(\n        f\"Timeliness metric computed successfully: Basic statistics -&gt; {(df[ColumnName.TIMELINESS.value].value_counts(),)}\",\n    )\n    return df\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Validity","title":"<code>Validity</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>class Validity(BaseMetric):\n    @timer\n    def compute_validity(self) -&gt; pl.DataFrame:\n        \"\"\"\n        Computes the validity metric for the dataset.\n\n        Returns:\n            pl.DataFrame: A DataFrame with an additional column indicating validity (1 for valid, 0 for invalid).\n        \"\"\"\n        if self.multiple_devices:\n            validity_df = self.df.with_columns(\n                pl.when(\n                    (pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value]) == 0)\n                    | (\n                        pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                        .diff()\n                        .fill_null(1)\n                        &lt; 0\n                    ).over(self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value])\n                )\n                .then(0)\n                .otherwise(1)\n                .alias(ColumnName.VALIDITY.value)\n            )\n        else:\n            validity_df = self.df.with_columns(\n                pl.when(\n                    (pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value]) == 0)\n                    | (\n                        pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                        .diff()\n                        .fill_null(0)\n                        &lt; 0\n                    )\n                )\n                .then(0)\n                .otherwise(1)\n                .alias(ColumnName.VALIDITY.value)\n            )\n        logger.info(\n            f\"Validity metric computed successfully: Basic statistics -&gt; {validity_df[ColumnName.VALIDITY.value].value_counts()}\",\n        )\n        return validity_df\n\n    def compute_metric(self) -&gt; pl.DataFrame:\n        \"\"\"\n        Computes the validity metric for each record.\n        This method evaluates the validity of a cumulative timeseries dataset, which is expected to have positive values.\n        It identifies periods where the data either drops to zero or exhibits a negative difference between consecutive observations.\n        \"\"\"\n        return self.compute_validity()\n\n    def compute_score(self) -&gt; pl.DataFrame:\n        \"\"\"\n        Computes the validity score for each records.\n        \"\"\"\n        return self.compute_metric()\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Validity.compute_metric","title":"<code>compute_metric()</code>","text":"<p>Computes the validity metric for each record. This method evaluates the validity of a cumulative timeseries dataset, which is expected to have positive values. It identifies periods where the data either drops to zero or exhibits a negative difference between consecutive observations.</p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>def compute_metric(self) -&gt; pl.DataFrame:\n    \"\"\"\n    Computes the validity metric for each record.\n    This method evaluates the validity of a cumulative timeseries dataset, which is expected to have positive values.\n    It identifies periods where the data either drops to zero or exhibits a negative difference between consecutive observations.\n    \"\"\"\n    return self.compute_validity()\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Validity.compute_score","title":"<code>compute_score()</code>","text":"<p>Computes the validity score for each records.</p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>def compute_score(self) -&gt; pl.DataFrame:\n    \"\"\"\n    Computes the validity score for each records.\n    \"\"\"\n    return self.compute_metric()\n</code></pre>"},{"location":"api/dqa/dimensions/#iot_dqa.dimensions.Validity.compute_validity","title":"<code>compute_validity()</code>","text":"<p>Computes the validity metric for the dataset.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A DataFrame with an additional column indicating validity (1 for valid, 0 for invalid).</p> Source code in <code>iot_dqa/dimensions.py</code> <pre><code>@timer\ndef compute_validity(self) -&gt; pl.DataFrame:\n    \"\"\"\n    Computes the validity metric for the dataset.\n\n    Returns:\n        pl.DataFrame: A DataFrame with an additional column indicating validity (1 for valid, 0 for invalid).\n    \"\"\"\n    if self.multiple_devices:\n        validity_df = self.df.with_columns(\n            pl.when(\n                (pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value]) == 0)\n                | (\n                    pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                    .diff()\n                    .fill_null(1)\n                    &lt; 0\n                ).over(self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value])\n            )\n            .then(0)\n            .otherwise(1)\n            .alias(ColumnName.VALIDITY.value)\n        )\n    else:\n        validity_df = self.df.with_columns(\n            pl.when(\n                (pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value]) == 0)\n                | (\n                    pl.col(self.col_mapping[ColumnMappingColumnName.VALUE.value])\n                    .diff()\n                    .fill_null(0)\n                    &lt; 0\n                )\n            )\n            .then(0)\n            .otherwise(1)\n            .alias(ColumnName.VALIDITY.value)\n        )\n    logger.info(\n        f\"Validity metric computed successfully: Basic statistics -&gt; {validity_df[ColumnName.VALIDITY.value].value_counts()}\",\n    )\n    return validity_df\n</code></pre>"},{"location":"api/dqa/dq_score/","title":"Dq score","text":""},{"location":"api/dqa/dq_score/#iot_dqa.dq_score.DataQualityScore","title":"<code>DataQualityScore</code>","text":"Source code in <code>iot_dqa/dq_score.py</code> <pre><code>class DataQualityScore:\n    def __init__(\n        self,\n        file_path: Union[str, None],\n        col_mapping: dict[str, str],\n        metrics_config: MetricsConfig = asdict(MetricsConfig()),\n        dimensions: list[Dimension] = [x.value for x in Dimension],\n        multiple_devices: bool = False,\n    ):\n        self.file_path = file_path\n        self.col_mapping = col_mapping\n        self.metrics_config = metrics_config\n        self.dimensions = dimensions\n        self.multiple_devices = multiple_devices\n\n    def _validate_col_mapping(self, df_cols: list[str]) -&gt; bool:\n        required_cols = {\n            ColumnMappingColumnName.DATE.value,\n            ColumnMappingColumnName.VALUE.value,\n        }\n\n        logger.info(\"Validating column mappings...\")\n        # Validate required columns\n        missing_required = required_cols - self.col_mapping.keys()\n        if missing_required:\n            raise InvalidColumnMappingException(\n                f\"The required columns: {', '.join(missing_required)} are not in the column mapping dictionary. Provide these keys and retry.\"\n            )\n\n        # Validate ID column if multiple devices is enabled\n        if self.multiple_devices:\n            if ColumnMappingColumnName.DEVICE_ID.value not in self.col_mapping.keys():\n                raise InvalidColumnMappingException(\n                    \"'id' is required when 'multiple_devices' is enabled. Provide it and retry.\"\n                )\n\n        # Validate column mapping values\n        invalid_values = [\n            v for v in self.col_mapping.values() if not isinstance(v, str)\n        ]\n        if invalid_values:\n            raise InvalidColumnMappingException(\n                f\"The following values should be strings: {', '.join(invalid_values)}\"\n            )\n\n        # Validate that column mapping values exist in the dataframe columns\n        missing_in_df = [v for v in self.col_mapping.values() if v not in df_cols]\n        if missing_in_df:\n            logger.error(\n                f\"The following columns are missing in the provided data: {', '.join(missing_in_df)}\"\n            )\n            raise InvalidColumnMappingException(\n                f\"The following columns are missing in the provided data: {', '.join(missing_in_df)}\"\n            )\n        logger.info(\"Column mapping validation completed without errrors...\")\n        return\n\n    def _validate_records(self, df_shape: int):\n        logger.info(\"Validating records...\")\n        if df_shape &lt; 50:\n            logger.error(\n                f\"The provided data ({df_shape}) records, is insufficient. At least 50 records are required in the CSV.\"\n            )\n            raise InsufficientDataException(\n                \"The provided data is insufficient. At least 50 records are required in the CSV.\"\n            )\n        logger.info(\"Record validation completed without errrors...\")\n        return\n\n    def _validate_dimensions(self):\n        logger.info(\"Validating dimensions...\")\n        supported_dimensions = [x.value for x in Dimension]\n        if isinstance(self.dimensions, list):\n            for dimension in self.dimensions:\n                if dimension.lower() not in supported_dimensions:\n                    logger.error(\n                        f\"The provided dimension: {dimension} is invalid. Only the following are supported:{supported_dimensions}\"\n                    )\n                    raise InvalidDimensionException(\n                        f\"The provided dimension: {dimension} is invalid. Only the following are supported:{supported_dimensions}\"\n                    )\n        logger.info(\"Dimension validation completed without errrors...\")\n        return\n\n    def _validate_config(self):\n        try:\n            logger.info(\"Validating metrics configuration...\")\n\n            self.metrics_config = MetricsConfig(\n                timeliness=TimelinessConfig(\n                    **(\n                        self.metrics_config.get(\"timeliness\")\n                        if self.metrics_config.get(\"timeliness\")\n                        else {}\n                    )\n                ),\n                accuracy=AccuracyConfig(\n                    **(\n                        self.metrics_config.get(\"accuracy\")\n                        if self.metrics_config.get(\"accuracy\")\n                        else {}\n                    )\n                ),\n                completeness_strategy=(\n                    self.metrics_config.get(\"completeness_strategy\")\n                    if self.metrics_config.get(\"completeness_strategy\")\n                    else CompletenessStrategy.ONLY_NULLS.value\n                ),\n            )\n            if self.metrics_config.accuracy.ensemble:\n                if len(self.metrics_config.accuracy.algorithms) &lt; 2:\n                    raise InvalidDimensionException(\n                        \"At least two outlier detection algorithms are required when ensemble is enabled.\"\n                    )\n            else:\n                if len(self.metrics_config.accuracy.algorithms) != 1:\n                    raise InvalidDimensionException(\n                        \"Exactly one outlier detection algorithm is required when ensemble is not enabled.\"\n                    )\n\n            logger.info(\"Metrics configuration validation completed without errors...\")\n\n            return\n        except Exception as e:\n            logger.error(\n                f\"An error occured during metrics configuration validation. Provided metrics: {self.metrics_config} -&gt; Error: {e} \"\n            )\n            raise e\n\n    def _data_loader(self) -&gt; pl.DataFrame:\n        \"\"\"Load the data from the CSV file using Polars.\n\n        Raises:\n            InvalidFileException: Raised when an invalid file is provided.\n\n        Returns:\n            pl.DataFrame: The Polars DataFrame object of the file.\n        \"\"\"\n        logger.info(\"Loading the data from the CSV file...\")\n        try:\n            df = pl.read_csv(self.file_path, infer_schema_length=100000)\n            logger.info(\"Data loaded successfully.\")\n            logger.info(f\"Data shape: {df.shape}\")\n            logger.info(f\"Data columns: {df.columns}\")\n            logger.info(f\"First few rows:\\n{df.head()}\")\n            return df\n        except Exception as e:\n            logger.error(\n                f\"An error occurred while loading the data from the CSV file. Error: {e}\"\n            )\n            raise InvalidFileException(\n                f\"The provided file is invalid. Ensure the path is valid and it is a valid CSV. Error: {e}\"\n            )\n\n    def _validate_weighting_mechanism(\n        self, weighting_mechanism: Union[WeightingMechanism, str]\n    ) -&gt; WeightingMechanism:\n        \"\"\"Validate the weighting mechanism.\n\n        Args:\n            weighting_mechanism (WeightingMechanism | str): The weighting mechanism to validate.\n\n        Raises:\n            InvalidDimensionException: Raised when an invalid weighting mechanism is provided.\n\n        Returns:\n            WeightingMechanism: The validated weighting mechanism.\n        \"\"\"\n        logger.info(\"Validating the weighting mechanism...\")\n        if isinstance(weighting_mechanism, str):\n            if weighting_mechanism not in [wm.value for wm in WeightingMechanism]:\n                raise InvalidDimensionException(\n                    f\"The provided weighting mechanism: {weighting_mechanism} is invalid. Only the following are supported: {[wm.value for wm in WeightingMechanism]}\"\n                )\n            return WeightingMechanism(weighting_mechanism)\n        logger.info(\"Weighting mechanism validation completed without errors...\")\n        return weighting_mechanism\n\n    def _validate_output_format(\n        self, output_format: Union[OutputFormat, str]\n    ) -&gt; OutputFormat:\n        \"\"\"Validate the output format.\n\n        Args:\n            output_format (OutputFormat | str): The output format to validate.\n\n        Raises:\n            InvalidDimensionException: Raised when an invalid output format is provided.\n\n        Returns:\n            OutputFormat: The validated output format.\n        \"\"\"\n        logger.info(\"Validating the output format...\")\n        if isinstance(output_format, str):\n            if output_format not in [v.value for v in OutputFormat]:\n                raise InvalidDimensionException(\n                    f\"The provided output format: {output_format} is invalid. Only the following are supported:{[v.value for v in OutputFormat]}\"\n                )\n            return OutputFormat(output_format)\n        logger.info(\"Output format validation completed without errors...\")\n        return output_format\n\n    def _validate_output_path(self, output_path: str):\n        \"\"\"Validate the output path.\n\n        Args:\n            output_path (str): The output path to validate.\n\n        Raises:\n            InvalidDimensionException: Raised when an invalid output path is provided.\n        \"\"\"\n        logger.info(\"Validating the output path...\")\n        if not isinstance(output_path, str):\n            raise InvalidDimensionException(\n                f\"The provided output path: {output_path} is invalid. It should be a string.\"\n            )\n        logger.info(\"Output path validation completed without errors...\")\n        return\n\n    def _validate_ahp_weights(self, ahp_weights: dict[str, float]):\n        \"\"\"Validate the AHP weights.\n\n        Args:\n            ahp_weights (dict[str, float]): The AHP weights to validate.\n\n        Raises:\n            InvalidDimensionException: Raised when invalid AHP weights are provided.\n        \"\"\"\n        logger.info(\"Validating the AHP weights...\")\n        if not isinstance(ahp_weights, dict):\n            raise InvalidDimensionException(\n                f\"The provided AHP weights: {ahp_weights} are invalid. They should be a dictionary.\"\n            )\n        if len(ahp_weights) != len(self.dimensions):\n            raise InvalidDimensionException(\n                f\"The provided AHP weights: {ahp_weights} are invalid. The number of weights should match the number of dimensions.\"\n            )\n\n        if len(ahp_weights) &gt; 4:\n            raise InvalidDimensionException(\n                f\"The provided AHP weights: {ahp_weights} are invalid. A maximum of 4 weights is allowed.\"\n            )\n        if not math.isclose(sum(ahp_weights.values()), 1.0, rel_tol=1e-9):\n            raise InvalidDimensionException(\n                f\"The provided AHP weights: {ahp_weights} are invalid. The weights must sum to 1.\"\n            )\n        logger.info(\"AHP weights validation completed without errors...\")\n        return\n\n    def compute_metrics(self):\n        # load file\n        df = self._data_loader()\n        # validations\n        self._validate_col_mapping(df.columns)\n        self._validate_records(df.shape[0])\n        self._validate_dimensions()\n        self._validate_config()\n\n        # select the necessary columns, incase the csv has many columns\n        df = df.select(list(self.col_mapping.values()))\n\n        logger.info(f\"Selected the necessary columns: {df.head()}\")\n\n        # sort the data by date and id\n        if self.multiple_devices:\n            df = df.sort(\n                by=[\n                    self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value],\n                    self.col_mapping[ColumnMappingColumnName.DATE.value],\n                ]\n            )\n        else:\n            df = df.sort(by=self.col_mapping[ColumnMappingColumnName.DATE.value])\n\n        df_metrics = df\n        # based on the selected dimensions, instantiate the classes.\n        if Dimension.VALIDITY.value in self.dimensions:\n            logger.info(\"Computing validity metric...\")\n\n            df_metrics = Validity(\n                df,\n                col_mapping=self.col_mapping,\n                multiple_devices=self.multiple_devices,\n                config=self.metrics_config,\n            ).compute_metric()\n\n            logger.info(\"Validity metric completed...\")\n            logger.info(f\"First few rows: {df_metrics.head()}\")\n\n        if Dimension.ACCURACY.value in self.dimensions:\n            logger.info(\"Computing accuracy metric...\")\n            df_metrics = Accuracy(\n                df_metrics,\n                col_mapping=self.col_mapping,\n                multiple_devices=self.multiple_devices,\n                config=self.metrics_config,\n            ).compute_metric()\n            logger.info(\"Accuracy metric completed...\")\n            logger.info(f\"First few rows: {df_metrics.head()}\")\n\n        if Dimension.COMPLETENESS.value in self.dimensions:\n            logger.info(\"Computing completeness metric...\")\n            df_metrics = Completeness(\n                df_metrics,\n                col_mapping=self.col_mapping,\n                multiple_devices=self.multiple_devices,\n                config=self.metrics_config,\n            ).compute_metric()\n            logger.info(\"Completeness metric completed...\")\n            logger.info(f\"First few rows: {df_metrics.head()}\")\n\n        if Dimension.TIMELINESS.value in self.dimensions:\n            logger.info(\"Computing timeliness metric...\")\n            df_metrics = Timeliness(\n                df_metrics,\n                col_mapping=self.col_mapping,\n                multiple_devices=self.multiple_devices,\n                config=self.metrics_config,\n            ).compute_metric()\n            logger.info(\"Timeliness metric completed...\")\n            logger.info(f\"First few rows: {df_metrics.head()}\")\n\n        return df_metrics\n\n    def compute_score(\n        self,\n        weighting_mechanism: str = WeightingMechanism.EQUAL.value,\n        output_format: Union[str, OutputFormat] = OutputFormat.CSV.value,\n        output_path: str = \"./output\",\n        ahp_weights: dict[str, float] = None,\n        export: bool = True,\n    ) -&gt; dict[str, dict[str, float]]:\n        \"\"\"Compute the score for each device and overall scores for all devices.\n\n        Args:\n            weighting_mechanism (str|WeightingMechanism, optional): The weighting mechanism to use for a single score. Defaults to \"Equal\".\n            output_format (str|OutputFormat, optional): The output format for the scores. Defaults to \"csv\".\n            output_path (str, optional): The path to save the output. Defaults to \"./output\".\n            ahp_weights (dict[str, float], optional): Weights for AHP computation. Required if weighting_mechanism is \"AHP\".\n\n        Returns:\n            dict: A dictionary containing scores per device and overall scores.\n        \"\"\"\n        logger.info(\"Computing the score for each device and overall scores...\")\n        # Ensure the metrics have been computed\n        df_with_metrics = self.compute_metrics()\n\n        # Validate the weighting mechanism\n        weighting_mechanism = self._validate_weighting_mechanism(weighting_mechanism)\n        # Validate the output format\n        output_format = self._validate_output_format(output_format)\n        # Validate the output path\n        self._validate_output_path(output_path)\n        # Validate the AHP weights if using AHP\n        if weighting_mechanism == WeightingMechanism.AHP:\n            self._validate_ahp_weights(ahp_weights)\n\n        # Compute the score for each dimension for each device\n        scores_per_device = {}\n        for device_id, group in df_with_metrics.group_by(\n            self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n        ):\n            device_id = device_id[0]\n            scores_per_device[device_id] = {\n                dimension: group[dimension].mean()\n                for dimension in self.dimensions\n                if dimension in group.columns\n            }\n\n        logger.info(f\"Scores per device: {scores_per_device}\")\n\n        # Compute the general score for each device based on the weighting mechanism\n        general_scores = {}\n        if weighting_mechanism == WeightingMechanism.EQUAL:\n            equal_weight = 1 / len(self.dimensions)\n            for device_id, scores in scores_per_device.items():\n                general_scores[device_id] = sum(\n                    scores[dim] * equal_weight for dim in scores\n                )\n        elif weighting_mechanism == WeightingMechanism.AHP:\n            for device_id, scores in scores_per_device.items():\n                general_scores[device_id] = sum(\n                    scores[dim] * ahp_weights.get(dim, 0) for dim in scores\n                )\n        else:\n            equal_weight = 1 / len(self.dimensions)\n            for device_id, scores in scores_per_device.items():\n                equal_score = sum(scores[dim] * equal_weight for dim in scores)\n                ahp_score = sum(scores[dim] * ahp_weights.get(dim, 0) for dim in scores)\n                general_scores[device_id] = (equal_score + ahp_score) / 2\n\n        # Add the general score to the scores per device\n        for device_id in scores_per_device:\n            scores_per_device[device_id][\"dqs\"] = general_scores[device_id]\n\n        logger.info(f\"Scores per device with dqs: {scores_per_device}\")\n\n        # Compute overall scores for all devices\n        overall_scores = {\n            dimension: sum(scores[dimension] for scores in scores_per_device.values())\n            / len(scores_per_device)\n            for dimension in self.dimensions\n        }\n        overall_scores[\"dqs\"] = sum(general_scores.values()) / len(general_scores)\n\n        logger.info(f\"Overall Data Quality Scores: {overall_scores}\")\n\n        # Ensure the output directory exists\n        try:\n            os.makedirs(output_path, exist_ok=True)\n        except Exception as e:\n            logger.error(\n                f\"An error occurred while creating the output directory: {output_path}. Error: {e}\"\n            )\n            raise e\n        if export:\n            if output_format == OutputFormat.CSV:\n                output_file_device = f\"{output_path}/scores_per_device.csv\"\n                output_file_overall = f\"{output_path}/overall_scores.csv\"\n\n                # Convert scores_per_device to a DataFrame for saving\n                scores_per_device_df = pl.DataFrame(\n                    [\n                        {\"device_id\": device_id, **scores}\n                        for device_id, scores in scores_per_device.items()\n                    ]\n                )\n                scores_per_device_df.write_csv(output_file_device)\n\n                # Save overall scores to a CSV file\n                overall_scores_df = pl.DataFrame([overall_scores])\n                overall_scores_df.write_csv(output_file_overall)\n\n                logger.info(f\"Scores per device saved to CSV at {output_file_device}\")\n                logger.info(f\"Overall scores saved to CSV at {output_file_overall}\")\n            else:\n                geojson_data = {\n                    \"type\": \"FeatureCollection\",\n                    \"features\": [\n                        {\n                            \"type\": \"Feature\",\n                            \"geometry\": {\n                                \"type\": \"Point\",\n                                \"coordinates\": [\n                                    df_with_metrics.filter(\n                                        pl.col(self.col_mapping[\"id\"]) == device_id\n                                    )[self.col_mapping[\"longitude\"]].to_list()[0],\n                                    df_with_metrics.filter(\n                                        pl.col(self.col_mapping[\"id\"]) == device_id\n                                    )[self.col_mapping[\"latitude\"]].to_list()[0],\n                                ],\n                            },\n                            \"properties\": {\n                                \"device_id\": device_id,\n                                **{\n                                    dimension: scores_per_device[device_id].get(\n                                        dimension, 0\n                                    )\n                                    for dimension in self.dimensions\n                                },\n                                \"dqs\": general_scores[device_id],\n                            },\n                        }\n                        for device_id in scores_per_device\n                    ],\n                }\n                output_file_device = f\"{output_path}/scores_per_device.geojson\"\n                output_file_overall = f\"{output_path}/overall_scores.json\"\n                with open(output_file_device, \"w\") as f:\n                    json.dump(geojson_data, f)\n                with open(output_file_overall, \"w\") as f:\n                    json.dump(overall_scores, f)\n                logger.info(\n                    f\"Scores per device saved to GeoJSON at {output_file_device}\"\n                )\n                logger.info(f\"Overall scores saved to JSON at {output_file_overall}\")\n\n        return {\n            \"scores_per_device\": scores_per_device,\n            \"general_scores\": general_scores,\n            \"overall_scores\": overall_scores,\n        }\n\n    def __repr__(self):\n        print(\"&lt;DataQualityScore&gt;\")\n</code></pre>"},{"location":"api/dqa/dq_score/#iot_dqa.dq_score.DataQualityScore.compute_score","title":"<code>compute_score(weighting_mechanism=WeightingMechanism.EQUAL.value, output_format=OutputFormat.CSV.value, output_path='./output', ahp_weights=None, export=True)</code>","text":"<p>Compute the score for each device and overall scores for all devices.</p> <p>Parameters:</p> Name Type Description Default <code>weighting_mechanism</code> <code>str | WeightingMechanism</code> <p>The weighting mechanism to use for a single score. Defaults to \"Equal\".</p> <code>EQUAL.value</code> <code>output_format</code> <code>str | OutputFormat</code> <p>The output format for the scores. Defaults to \"csv\".</p> <code>CSV.value</code> <code>output_path</code> <code>str</code> <p>The path to save the output. Defaults to \"./output\".</p> <code>'./output'</code> <code>ahp_weights</code> <code>dict[str, float]</code> <p>Weights for AHP computation. Required if weighting_mechanism is \"AHP\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, dict[str, float]]</code> <p>A dictionary containing scores per device and overall scores.</p> Source code in <code>iot_dqa/dq_score.py</code> <pre><code>def compute_score(\n    self,\n    weighting_mechanism: str = WeightingMechanism.EQUAL.value,\n    output_format: Union[str, OutputFormat] = OutputFormat.CSV.value,\n    output_path: str = \"./output\",\n    ahp_weights: dict[str, float] = None,\n    export: bool = True,\n) -&gt; dict[str, dict[str, float]]:\n    \"\"\"Compute the score for each device and overall scores for all devices.\n\n    Args:\n        weighting_mechanism (str|WeightingMechanism, optional): The weighting mechanism to use for a single score. Defaults to \"Equal\".\n        output_format (str|OutputFormat, optional): The output format for the scores. Defaults to \"csv\".\n        output_path (str, optional): The path to save the output. Defaults to \"./output\".\n        ahp_weights (dict[str, float], optional): Weights for AHP computation. Required if weighting_mechanism is \"AHP\".\n\n    Returns:\n        dict: A dictionary containing scores per device and overall scores.\n    \"\"\"\n    logger.info(\"Computing the score for each device and overall scores...\")\n    # Ensure the metrics have been computed\n    df_with_metrics = self.compute_metrics()\n\n    # Validate the weighting mechanism\n    weighting_mechanism = self._validate_weighting_mechanism(weighting_mechanism)\n    # Validate the output format\n    output_format = self._validate_output_format(output_format)\n    # Validate the output path\n    self._validate_output_path(output_path)\n    # Validate the AHP weights if using AHP\n    if weighting_mechanism == WeightingMechanism.AHP:\n        self._validate_ahp_weights(ahp_weights)\n\n    # Compute the score for each dimension for each device\n    scores_per_device = {}\n    for device_id, group in df_with_metrics.group_by(\n        self.col_mapping[ColumnMappingColumnName.DEVICE_ID.value]\n    ):\n        device_id = device_id[0]\n        scores_per_device[device_id] = {\n            dimension: group[dimension].mean()\n            for dimension in self.dimensions\n            if dimension in group.columns\n        }\n\n    logger.info(f\"Scores per device: {scores_per_device}\")\n\n    # Compute the general score for each device based on the weighting mechanism\n    general_scores = {}\n    if weighting_mechanism == WeightingMechanism.EQUAL:\n        equal_weight = 1 / len(self.dimensions)\n        for device_id, scores in scores_per_device.items():\n            general_scores[device_id] = sum(\n                scores[dim] * equal_weight for dim in scores\n            )\n    elif weighting_mechanism == WeightingMechanism.AHP:\n        for device_id, scores in scores_per_device.items():\n            general_scores[device_id] = sum(\n                scores[dim] * ahp_weights.get(dim, 0) for dim in scores\n            )\n    else:\n        equal_weight = 1 / len(self.dimensions)\n        for device_id, scores in scores_per_device.items():\n            equal_score = sum(scores[dim] * equal_weight for dim in scores)\n            ahp_score = sum(scores[dim] * ahp_weights.get(dim, 0) for dim in scores)\n            general_scores[device_id] = (equal_score + ahp_score) / 2\n\n    # Add the general score to the scores per device\n    for device_id in scores_per_device:\n        scores_per_device[device_id][\"dqs\"] = general_scores[device_id]\n\n    logger.info(f\"Scores per device with dqs: {scores_per_device}\")\n\n    # Compute overall scores for all devices\n    overall_scores = {\n        dimension: sum(scores[dimension] for scores in scores_per_device.values())\n        / len(scores_per_device)\n        for dimension in self.dimensions\n    }\n    overall_scores[\"dqs\"] = sum(general_scores.values()) / len(general_scores)\n\n    logger.info(f\"Overall Data Quality Scores: {overall_scores}\")\n\n    # Ensure the output directory exists\n    try:\n        os.makedirs(output_path, exist_ok=True)\n    except Exception as e:\n        logger.error(\n            f\"An error occurred while creating the output directory: {output_path}. Error: {e}\"\n        )\n        raise e\n    if export:\n        if output_format == OutputFormat.CSV:\n            output_file_device = f\"{output_path}/scores_per_device.csv\"\n            output_file_overall = f\"{output_path}/overall_scores.csv\"\n\n            # Convert scores_per_device to a DataFrame for saving\n            scores_per_device_df = pl.DataFrame(\n                [\n                    {\"device_id\": device_id, **scores}\n                    for device_id, scores in scores_per_device.items()\n                ]\n            )\n            scores_per_device_df.write_csv(output_file_device)\n\n            # Save overall scores to a CSV file\n            overall_scores_df = pl.DataFrame([overall_scores])\n            overall_scores_df.write_csv(output_file_overall)\n\n            logger.info(f\"Scores per device saved to CSV at {output_file_device}\")\n            logger.info(f\"Overall scores saved to CSV at {output_file_overall}\")\n        else:\n            geojson_data = {\n                \"type\": \"FeatureCollection\",\n                \"features\": [\n                    {\n                        \"type\": \"Feature\",\n                        \"geometry\": {\n                            \"type\": \"Point\",\n                            \"coordinates\": [\n                                df_with_metrics.filter(\n                                    pl.col(self.col_mapping[\"id\"]) == device_id\n                                )[self.col_mapping[\"longitude\"]].to_list()[0],\n                                df_with_metrics.filter(\n                                    pl.col(self.col_mapping[\"id\"]) == device_id\n                                )[self.col_mapping[\"latitude\"]].to_list()[0],\n                            ],\n                        },\n                        \"properties\": {\n                            \"device_id\": device_id,\n                            **{\n                                dimension: scores_per_device[device_id].get(\n                                    dimension, 0\n                                )\n                                for dimension in self.dimensions\n                            },\n                            \"dqs\": general_scores[device_id],\n                        },\n                    }\n                    for device_id in scores_per_device\n                ],\n            }\n            output_file_device = f\"{output_path}/scores_per_device.geojson\"\n            output_file_overall = f\"{output_path}/overall_scores.json\"\n            with open(output_file_device, \"w\") as f:\n                json.dump(geojson_data, f)\n            with open(output_file_overall, \"w\") as f:\n                json.dump(overall_scores, f)\n            logger.info(\n                f\"Scores per device saved to GeoJSON at {output_file_device}\"\n            )\n            logger.info(f\"Overall scores saved to JSON at {output_file_overall}\")\n\n    return {\n        \"scores_per_device\": scores_per_device,\n        \"general_scores\": general_scores,\n        \"overall_scores\": overall_scores,\n    }\n</code></pre>"},{"location":"api/utils/configs/","title":"Configs","text":""},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig","title":"<code>AccuracyConfig</code>  <code>dataclass</code>","text":"<p>Configuration class for completeness settings. Methods:     post_init(): Validates the provided algorithms and ensemble flag.</p> Source code in <code>iot_dqa/utils/configs.py</code> <pre><code>@dataclass\nclass AccuracyConfig:\n    \"\"\"\n    Configuration class for completeness settings.\n    Methods:\n        __post_init__(): Validates the provided algorithms and ensemble flag.\n    \"\"\"\n\n    ensemble: bool = True\n    \"\"\"Flag to indicate if ensemble methods should be used. Default is True.\"\"\"\n    mad_threshold: int = 3\n    \"\"\" Threshold for Median Absolute Deviation (MAD). Default is 3. Using 3 * STD as decribed in the literature.\"\"\"\n    optimize_iqr_with_optuna: bool = True\n    \"\"\"Flag to indicate if IQR optimization should be performed using optuna. Default is True.\"\"\"\n    iqr_optuna_trials: Union[int, None] = 10\n    \"\"\"10 trials when optimizing the IQR\"\"\"\n    iqr_optuna_q1_min: Union[int, None] = 0\n    \"\"\"Minimum value for the first quartile (Q1) in IQR optimization. Default is 0.0.\"\"\"\n    iqr_optuna_q1_max: Union[float, None] = 0.5\n    \"\"\"Maximum value for the first quartile (Q1) in IQR optimization. Default is 0.5.\"\"\"\n    iqr_optuna_q3_min: Union[float, None] = 0.5\n    \"\"\"Minimum value for the third quartile (Q3) in IQR optimization. Default is 0.5.\"\"\"\n    iqr_optuna_q3_max: Union[int, None] = 1\n    \"\"\"Maximum value for the third quartile (Q3) in IQR optimization. Default is 1.0.\"\"\"\n\n    algorithms: list[OutlierDetectionAlgorithm] = field(\n        default_factory=lambda: [x.value for x in OutlierDetectionAlgorithm]\n    )\n    \"\"\"List of outlier detection algorithms to be used. Default is all values of OutlierDetectionAlgorithm.\"\"\"\n    strategy: AccuracyStrategy = AccuracyStrategy.NONE.value\n    \"\"\"Determine the approach to use for the accuracy computation.\"\"\"\n    isolation_forest: IsolationForestConfig = field(\n        default_factory=IsolationForestConfig\n    )\n    \"\"\"Configuration for Isolation Forest settings.\"\"\"\n\n    def __post_init__(self):\n        if not all(\n            algo in OutlierDetectionAlgorithm._value2member_map_\n            for algo in self.algorithms\n        ):\n            raise ValueError(\n                f\"All algorithms must be valid values of OutlierDetectionAlgorithm. Provided: {self.algorithms}\"\n            )\n        if not isinstance(self.ensemble, bool):\n            raise ValueError(\n                f\"Ensemble must be valid boolean. Provided: {self.ensemble}\"\n            )\n\n        if not isinstance(self.optimize_iqr_with_optuna, bool):\n            raise ValueError(\n                f\"IQR optimization must be valid boolean. Provided: {self.optimize_iqr_with_optuna}\"\n            )\n        if not isinstance(self.mad_threshold, int):\n            raise ValueError(\n                f\"MAD threshold must be valid boolean. Provided: {self.mad_threshold}\"\n            )\n        if not isinstance(self.iqr_optuna_trials, int):\n            raise ValueError(\n                f\"IQR optuna trial must be valid integer. Provided: {self.iqr_optuna_trials}\"\n            )\n\n        if not isinstance(self.iqr_optuna_q1_min, int):\n            raise ValueError(\n                f\"IQR optuna Q1 min must be valid integer. Provided: {self.iqr_optuna_q1_min}\"\n            )\n\n        if not isinstance(self.iqr_optuna_q1_max, float):\n            raise ValueError(\n                f\"IQR optuna Q1 max must be valid float. Provided: {self.iqr_optuna_q1_max}\"\n            )\n\n        if not isinstance(self.iqr_optuna_q3_min, float):\n            raise ValueError(\n                f\"IQR optuna Q1 min must be valid float. Provided: {self.iqr_optuna_q3_min}\"\n            )\n\n        if not isinstance(self.iqr_optuna_q3_max, int):\n            raise ValueError(\n                f\"IQR optuna Q1 max must be valid integer. Provided: {self.iqr_optuna_q3_max}\"\n            )\n\n        if self.iqr_optuna_q3_max &gt; 1:\n            raise ValueError(\n                f\"IQR Q3 max must be less than or equal to 1. Provided: {self.iqr_optuna_q3_max}\"\n            )\n        if self.iqr_optuna_q1_min &lt; 0:\n            raise ValueError(\n                f\"IQR Q1 min must be greater than or equal to 0. Provided: {self.iqr_optuna_q1_min}\"\n            )\n</code></pre>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig.algorithms","title":"<code>algorithms: list[OutlierDetectionAlgorithm] = field(default_factory=lambda: [x.value for x in OutlierDetectionAlgorithm])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of outlier detection algorithms to be used. Default is all values of OutlierDetectionAlgorithm.</p>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig.ensemble","title":"<code>ensemble: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Flag to indicate if ensemble methods should be used. Default is True.</p>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig.iqr_optuna_q1_max","title":"<code>iqr_optuna_q1_max: Union[float, None] = 0.5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum value for the first quartile (Q1) in IQR optimization. Default is 0.5.</p>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig.iqr_optuna_q1_min","title":"<code>iqr_optuna_q1_min: Union[int, None] = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum value for the first quartile (Q1) in IQR optimization. Default is 0.0.</p>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig.iqr_optuna_q3_max","title":"<code>iqr_optuna_q3_max: Union[int, None] = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum value for the third quartile (Q3) in IQR optimization. Default is 1.0.</p>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig.iqr_optuna_q3_min","title":"<code>iqr_optuna_q3_min: Union[float, None] = 0.5</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum value for the third quartile (Q3) in IQR optimization. Default is 0.5.</p>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig.iqr_optuna_trials","title":"<code>iqr_optuna_trials: Union[int, None] = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>10 trials when optimizing the IQR</p>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig.isolation_forest","title":"<code>isolation_forest: IsolationForestConfig = field(default_factory=IsolationForestConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Configuration for Isolation Forest settings.</p>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig.mad_threshold","title":"<code>mad_threshold: int = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Threshold for Median Absolute Deviation (MAD). Default is 3. Using 3 * STD as decribed in the literature.</p>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig.optimize_iqr_with_optuna","title":"<code>optimize_iqr_with_optuna: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Flag to indicate if IQR optimization should be performed using optuna. Default is True.</p>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.AccuracyConfig.strategy","title":"<code>strategy: AccuracyStrategy = AccuracyStrategy.NONE.value</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Determine the approach to use for the accuracy computation.</p>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.IsolationForestConfig","title":"<code>IsolationForestConfig</code>  <code>dataclass</code>","text":"<p>Configuration class for Isolation Forest settings. Attributes:     n_estimators (int): Number of trees in the forest. Default is 100.     max_samples (float): Number of samples to draw to train each base estimator. Default is 0.8.     contamination (float): Proportion of outliers in the data set. Default is 0.1.     max_features (int): Number of features to draw to train each base estimator. Default is 1.</p> Source code in <code>iot_dqa/utils/configs.py</code> <pre><code>@dataclass\nclass IsolationForestConfig:\n    \"\"\"\n    Configuration class for Isolation Forest settings.\n    Attributes:\n        n_estimators (int): Number of trees in the forest. Default is 100.\n        max_samples (float): Number of samples to draw to train each base estimator. Default is 0.8.\n        contamination (float): Proportion of outliers in the data set. Default is 0.1.\n        max_features (int): Number of features to draw to train each base estimator. Default is 1.\n    \"\"\"\n\n    n_estimators: int = 100\n    max_samples: float = 0.8\n    contamination: Union[float, str] = 0.1\n    max_features: int = 1\n    random_state: int = 42\n</code></pre>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.TimelinessConfig","title":"<code>TimelinessConfig</code>  <code>dataclass</code>","text":"<p>Configuration class for timeliness settings. Attributes:     frequency (str): Frequency of the data. Default is \"1H\".     iat_method (str): Method to calculate inter-arrival time. Default is \"mean\".</p> Source code in <code>iot_dqa/utils/configs.py</code> <pre><code>@dataclass\nclass TimelinessConfig:\n    \"\"\"\n    Configuration class for timeliness settings.\n    Attributes:\n        frequency (str): Frequency of the data. Default is \"1H\".\n        iat_method (str): Method to calculate inter-arrival time. Default is \"mean\".\n    \"\"\"\n\n    iat_method: FrequencyCalculationMethod = FrequencyCalculationMethod.MIN.value\n    \"\"\"Method to calculate inter-arrival time. Default is 'min'.\"\"\"\n\n    def __post_init__(self):\n        if not (self.iat_method):\n            raise ValueError(\n                \"At least one of 'frequency_unit' or 'frequency_unit' or 'iat_method' must be provided.\"\n            )\n\n        if not isinstance(self.iat_method, str):\n            raise ValueError(\n                f\"IAT method must be valid string. Provided: {self.iat_method}\"\n            )\n</code></pre>"},{"location":"api/utils/configs/#iot_dqa.utils.configs.TimelinessConfig.iat_method","title":"<code>iat_method: FrequencyCalculationMethod = FrequencyCalculationMethod.MIN.value</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Method to calculate inter-arrival time. Default is 'min'.</p>"},{"location":"api/utils/enums/","title":"Enums","text":""},{"location":"api/utils/enums/#supported-dimensions","title":"Supported Dimensions","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>iot_dqa/utils/enums.py</code> <pre><code>class Dimension(enum.Enum):\n    COMPLETENESS = \"completeness\"\n    ACCURACY = \"accuracy\"\n    TIMELINESS = \"timeliness\"\n    VALIDITY = \"validity\"\n</code></pre>"},{"location":"api/utils/enums/#weighting-mechanism","title":"Weighting Mechanism","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>iot_dqa/utils/enums.py</code> <pre><code>class WeightingMechanism(enum.Enum):\n    EQUAL = \"equal\"\n    AHP = \"ahp\"\n    BOTH = \"both\"\n    \"\"\"Equal weighting or AHP weighting or both.\"\"\"\n</code></pre>"},{"location":"api/utils/enums/#iot_dqa.utils.enums.WeightingMechanism.BOTH","title":"<code>BOTH = 'both'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Equal weighting or AHP weighting or both.</p>"},{"location":"api/utils/enums/#outlier-detection-algorithm","title":"Outlier Detection Algorithm","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>iot_dqa/utils/enums.py</code> <pre><code>class OutlierDetectionAlgorithm(enum.Enum):\n    IQR = \"IQR\"\n    \"\"\"Interquatile Range\"\"\"\n    MAD = \"MAD\"\n    \"\"\"Median Absolute Deviation\"\"\"\n    IF = \"IF\"\n    \"\"\"Isolation Forest\"\"\"\n</code></pre>"},{"location":"api/utils/enums/#iot_dqa.utils.enums.OutlierDetectionAlgorithm.IF","title":"<code>IF = 'IF'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Isolation Forest</p>"},{"location":"api/utils/enums/#iot_dqa.utils.enums.OutlierDetectionAlgorithm.IQR","title":"<code>IQR = 'IQR'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Interquatile Range</p>"},{"location":"api/utils/enums/#iot_dqa.utils.enums.OutlierDetectionAlgorithm.MAD","title":"<code>MAD = 'MAD'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Median Absolute Deviation</p>"},{"location":"api/utils/enums/#frequency-calculation-method","title":"Frequency Calculation Method","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>iot_dqa/utils/enums.py</code> <pre><code>class FrequencyCalculationMethod(enum.Enum):\n    MIN = \"min\"\n    \"\"\"Minimum Inter Arrival Time (IAT).\"\"\"\n    MODE = \"mode\"\n    \"\"\"Mode of Inter Arrival Time (IAT).\"\"\"\n</code></pre>"},{"location":"api/utils/enums/#iot_dqa.utils.enums.FrequencyCalculationMethod.MIN","title":"<code>MIN = 'min'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum Inter Arrival Time (IAT).</p>"},{"location":"api/utils/enums/#iot_dqa.utils.enums.FrequencyCalculationMethod.MODE","title":"<code>MODE = 'mode'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mode of Inter Arrival Time (IAT).</p>"},{"location":"api/utils/enums/#output-format","title":"Output Format","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>iot_dqa/utils/enums.py</code> <pre><code>class OutputFormat(enum.Enum):\n    CSV = \"csv\"\n    \"\"\"Comma Separated Values\"\"\"\n    GEOJSON = \"geojson\"\n    \"\"\"JavaScript Object Notation\"\"\"\n</code></pre>"},{"location":"api/utils/enums/#iot_dqa.utils.enums.OutputFormat.CSV","title":"<code>CSV = 'csv'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Comma Separated Values</p>"},{"location":"api/utils/enums/#iot_dqa.utils.enums.OutputFormat.GEOJSON","title":"<code>GEOJSON = 'geojson'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>JavaScript Object Notation</p>"},{"location":"api/utils/enums/#completeness-strategy","title":"Completeness Strategy","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>iot_dqa/utils/enums.py</code> <pre><code>class CompletenessStrategy(enum.Enum):\n    ONLY_NULLS = \"nulls\"\n    \"\"\"Consider missing/nulls as incomplete.\"\"\"\n    ACCURACY = \"accuracy\"\n    \"\"\"Consider only accurate values as complete.\"\"\"\n    TIMELINESS = \"timeliness\"\n    \"\"\"Checks the IAT of the device to compute the expected records vs the sent records.\"\"\"\n</code></pre>"},{"location":"api/utils/enums/#iot_dqa.utils.enums.CompletenessStrategy.ACCURACY","title":"<code>ACCURACY = 'accuracy'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Consider only accurate values as complete.</p>"},{"location":"api/utils/enums/#iot_dqa.utils.enums.CompletenessStrategy.ONLY_NULLS","title":"<code>ONLY_NULLS = 'nulls'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Consider missing/nulls as incomplete.</p>"},{"location":"api/utils/enums/#iot_dqa.utils.enums.CompletenessStrategy.TIMELINESS","title":"<code>TIMELINESS = 'timeliness'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Checks the IAT of the device to compute the expected records vs the sent records.</p>"},{"location":"api/utils/enums/#accuracy-strategy","title":"Accuracy Strategy","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>iot_dqa/utils/enums.py</code> <pre><code>class AccuracyStrategy(enum.Enum):\n    VALIDITY = \"v\"\n    NONE = \"none\"\n</code></pre>"},{"location":"api/utils/exceptions/","title":"Exceptions","text":""},{"location":"api/utils/exceptions/#exceptions","title":"Exceptions","text":"<p>The following exceptions are defined in the <code>iot_dqa.utils.exceptions</code> module:</p>"},{"location":"api/utils/exceptions/#iot_dqa.utils.exceptions.InsufficientDataException","title":"<code>InsufficientDataException</code>","text":"<p>               Bases: <code>IoTDataQualityAssessmentBaseException</code></p> <p>Raised when the provided data is insufficient.</p> Source code in <code>iot_dqa/utils/exceptions.py</code> <pre><code>class InsufficientDataException(IoTDataQualityAssessmentBaseException):\n    \"\"\"Raised when the provided data is insufficient.\"\"\"\n</code></pre>"},{"location":"api/utils/exceptions/#iot_dqa.utils.exceptions.InvalidColumnMappingException","title":"<code>InvalidColumnMappingException</code>","text":"<p>               Bases: <code>IoTDataQualityAssessmentBaseException</code></p> <p>Raised when an invalid column mapping is provided.</p> Source code in <code>iot_dqa/utils/exceptions.py</code> <pre><code>class InvalidColumnMappingException(IoTDataQualityAssessmentBaseException):\n    \"\"\"Raised when an invalid column mapping is provided.\"\"\"\n</code></pre>"},{"location":"api/utils/exceptions/#iot_dqa.utils.exceptions.InvalidDimensionException","title":"<code>InvalidDimensionException</code>","text":"<p>               Bases: <code>IoTDataQualityAssessmentBaseException</code></p> <p>Raised when an invalid dimension is provided.</p> Source code in <code>iot_dqa/utils/exceptions.py</code> <pre><code>class InvalidDimensionException(IoTDataQualityAssessmentBaseException):\n    \"\"\"Raised when an invalid dimension is provided.\"\"\"\n</code></pre>"},{"location":"api/utils/exceptions/#iot_dqa.utils.exceptions.InvalidFileException","title":"<code>InvalidFileException</code>","text":"<p>               Bases: <code>IoTDataQualityAssessmentBaseException</code></p> <p>Raised when an invalid file is provided.</p> Source code in <code>iot_dqa/utils/exceptions.py</code> <pre><code>class InvalidFileException(IoTDataQualityAssessmentBaseException):\n    \"\"\"Raised when an invalid file is provided.\"\"\"\n</code></pre>"},{"location":"api/utils/logger/","title":"Logger","text":""},{"location":"api/utils/logger/#iot_dqa.utils.logger.add_file_logging","title":"<code>add_file_logging(file_path='iot-dqa.log')</code>","text":"<p>Add file logging to the logger.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the log file. Defaults to 'iot-dqa.log'.</p> <code>'iot-dqa.log'</code> Source code in <code>iot_dqa/utils/logger.py</code> <pre><code>def add_file_logging(file_path=\"iot-dqa.log\"):\n    \"\"\"\n    Add file logging to the logger.\n\n    Args:\n        file_path (str): Path to the log file. Defaults to 'iot-dqa.log'.\n    \"\"\"\n    file_handler = logging.FileHandler(file_path)\n    file_formatter = logging.Formatter(\n        \"iot-dqa:%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    )\n    file_handler.setFormatter(file_formatter)\n    logger.addHandler(file_handler)\n</code></pre>"},{"location":"api/utils/logger/#iot_dqa.utils.logger.configure_logging","title":"<code>configure_logging(level=logging.WARNING)</code>","text":"<p>Configure the logging level for the package.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Logging level (e.g., logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR)</p> <code>WARNING</code> Example Source code in <code>iot_dqa/utils/logger.py</code> <pre><code>def configure_logging(level=logging.WARNING):\n    \"\"\"\n    Configure the logging level for the package.\n\n    Args:\n        level (int): Logging level (e.g., logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR)\n\n    Example:\n        # Set logging level to INFO\n        configure_logging(logging.INFO)\n    \"\"\"\n    logger.setLevel(level)\n    for handler in logger.handlers:\n        handler.setLevel(level)\n</code></pre>"},{"location":"api/utils/logger/#iot_dqa.utils.logger.configure_logging--set-logging-level-to-info","title":"Set logging level to INFO","text":"<p>configure_logging(logging.INFO)</p>"},{"location":"examples/logging/","title":"Logging","text":"<p>This document provides examples for updating the default logging configuration in the <code>iot-dqa</code> package.</p>"},{"location":"examples/logging/#adding-file-logging","title":"Adding File Logging","text":"<p>You can log messages to a file using the <code>add_file_logging</code> function:</p> <pre><code>from iot_dqa.utils import add_file_logging\n# Example usage:\nadd_file_logging(\"custom-log-file.log\")\n</code></pre>"},{"location":"examples/logging/#configuring-logging-levels","title":"Configuring Logging Levels","text":"<p>You can configure the logging level for the package using the <code>configure_logging</code> function:</p> <pre><code>from iot_dqa.utils import configure_logging\n# Example usage:\nconfigure_logging(logging.DEBUG)  # Set logging level to DEBUG\n</code></pre>"},{"location":"examples/usage/","title":"Example Usage","text":""},{"location":"examples/usage/#example-1-compute-a-few-metrics","title":"Example 1: Compute a Few Metrics","text":"<p>To compute specific metrics, specify the dimensions you want:</p> <pre><code>from iot_dqa.utils.enums import Dimension\n\ndqs = DataQualityScore(\n    file_path=\"path/to/your/data.csv\",\n    col_mapping={\n        \"date\": \"date_column_name\",\n        \"value\": \"value_column_name\",\n    },\n    dimensions=[Dimension.VALIDITY.value, Dimension.TIMELINESS.value],\n)\n\nmetrics = dqs.compute_metrics()\nprint(metrics)\n</code></pre>"},{"location":"examples/usage/#example-2-adjust-configuration","title":"Example 2: Adjust Configuration","text":"<p>You can adjust the configuration for metrics computation:</p> <pre><code>from iot_dqa.utils.configs import MetricsConfig, AccuracyConfig\n\ncustom_config = MetricsConfig(\n    accuracy=AccuracyConfig(ensemble=True, algorithms=[\"z_score\", \"iqr\"])\n)\n\ndqs = DataQualityScore(\n    file_path=\"path/to/your/data.csv\",\n    col_mapping={\n        \"date\": \"date_column_name\",\n        \"value\": \"value_column_name\",\n    },\n    metrics_config=custom_config,\n)\n\nmetrics = dqs.compute_metrics()\nprint(metrics)\n</code></pre>"},{"location":"examples/usage/#example-3-compute-score","title":"Example 3: Compute Score","text":"<p>To compute the overall data quality score:</p> <pre><code>from iot_dqa.utils.enums import WeightingMechanism, OutputFormat\n\ndqs = DataQualityScore(\n    file_path=\"path/to/your/data.csv\",\n    col_mapping={\n        \"date\": \"date_column_name\",\n        \"value\": \"value_column_name\",\n        \"id\": \"device_id_column_name\",\n    },\n)\n\nscores = dqs.compute_score(\n    weighting_mechanism=WeightingMechanism.EQUAL.value,\n    output_format=OutputFormat.CSV.value,\n    output_path=\"./output\",\n)\nprint(scores)\n</code></pre>"},{"location":"examples/usage/#example-4-export-score-to-filegeojsoncsv","title":"Example 4: Export Score to File/GeoJSON/CSV","text":"<p>To export the computed scores to a file:</p> <pre><code>dqs = DataQualityScore(\n    file_path=\"path/to/your/data.csv\",\n    col_mapping={\n        \"date\": \"date_column_name\",\n        \"value\": \"value_column_name\",\n        \"id\": \"device_id_column_name\",\n    },\n)\n\nscores = dqs.compute_score(\n    output_format=\"geojson\",  # Options: \"csv\", \"geojson\"\n    output_path=\"./output\",\n    export=True,\n)\nprint(\"Scores exported successfully.\")\n</code></pre>"},{"location":"examples/usage/#example-5-ahp-weighting-example","title":"Example 5: AHP Weighting Example","text":"<p>To compute the data quality score using AHP (Analytic Hierarchy Process) weighting:</p> <pre><code>from iot_dqa.utils.enums import WeightingMechanism\n\ndqs = DataQualityScore(\n    file_path=\"path/to/your/data.csv\",\n    col_mapping={\n        \"date\": \"date_column_name\",\n        \"value\": \"value_column_name\",\n        \"id\": \"device_id_column_name\",\n    },\n)\n\nahp_weights = {\n    \"validity\": 0.4,\n    \"accuracy\": 0.3,\n    \"completeness\": 0.2,\n    \"timeliness\": 0.1,\n}\n\nscores = dqs.compute_score(\n    weighting_mechanism=WeightingMechanism.AHP.value,\n    ahp_weights=ahp_weights,\n    output_format=\"csv\",\n    output_path=\"./output\",\n)\nprint(scores)\n</code></pre>"},{"location":"examples/usage/#example-6-isolation-forest-for-outlier-detection","title":"Example 6: Isolation Forest for Outlier Detection","text":"<pre><code>from iot_dqa.utils.configs import MetricsConfig, AccuracyConfig\nfrom sklearn.ensemble import IsolationForest\n\n# Define custom metrics configuration with Isolation Forest\ncustom_metrics_config = MetricsConfig(\n    accuracy=AccuracyConfig(\n        ensemble=False,\n        algorithms=[\"if\"],\n        isolation_forest={\"n_estimators\": 100, \"max_samples\": \"auto\", \"random_state\": 42}\n    )\n)\n\n# Initialize DataQualityScore with Isolation Forest configuration\ndqs = DataQualityScore(\n    file_path=\"data/sample.csv\",\n    col_mapping={\"date\": \"timestamp\", \"value\": \"sensor_value\", \"id\": \"device_id\"},\n    metrics_config=custom_metrics_config,\n    dimensions=[\"accuracy\"]\n)\n\n# Compute metrics\nmetrics = dqs.compute_metrics()\nprint(metrics)\n</code></pre>"},{"location":"examples/usage/#example-7-timeliness-with-custom-inter-arrival-time-method","title":"Example 7: Timeliness with Custom Inter-Arrival Time Method","text":"<pre><code>from iot_dqa.utils.configs import MetricsConfig, TimelinessConfig\nfrom iot_dqa.utils.enums import FrequencyCalculationMethod\n\n# Define custom metrics configuration for timeliness\ncustom_metrics_config = MetricsConfig(\n    timeliness=TimelinessConfig(\n        iat_method=FrequencyCalculationMethod.MODE.value\n    )\n)\n\n# Initialize DataQualityScore with custom timeliness configuration\ndqs = DataQualityScore(\n    file_path=\"data/sample.csv\",\n    col_mapping={\"date\": \"timestamp\", \"value\": \"sensor_value\", \"id\": \"device_id\"},\n    metrics_config=custom_metrics_config,\n    dimensions=[\"timeliness\"],\n    multiple_devices=True\n)\n\n# Compute metrics\nmetrics = dqs.compute_metrics()\nprint(metrics)\n</code></pre>"},{"location":"examples/usage/#example-8-inter-quartile-range-iqr-with-optuna-optimization","title":"Example 8: Inter-Quartile Range (IQR) with Optuna Optimization","text":"<pre><code>from iot_dqa.utils.configs import MetricsConfig, AccuracyConfig\n\n# Define custom metrics configuration with IQR and Optuna optimization\ncustom_metrics_config = MetricsConfig(\n    accuracy=AccuracyConfig(\n        ensemble=False,\n        algorithms=[\"iqr\"],\n        optimize_iqr_with_optuna=True,\n        iqr_optuna_trials=50,\n        iqr_optuna_q1_min=0.1,\n        iqr_optuna_q1_max=0.3,\n        iqr_optuna_q3_min=0.7,\n        iqr_optuna_q3_max=0.9\n    )\n)\n\n# Initialize DataQualityScore with IQR configuration\ndqs = DataQualityScore(\n    file_path=\"data/sample.csv\",\n    col_mapping={\"date\": \"timestamp\", \"value\": \"sensor_value\", \"id\": \"device_id\"},\n    metrics_config=custom_metrics_config,\n    dimensions=[\"accuracy\"]\n)\n\n# Compute metrics\nmetrics = dqs.compute_metrics()\nprint(metrics)\n</code></pre>"}]}